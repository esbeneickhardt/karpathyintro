{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e79d4c9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca632e",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adadc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41ab87",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ac541",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading names into a list\n",
    "with open('../../data/names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce664c",
   "metadata": {},
   "source": [
    "### Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df235e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabulary (character to/from index)\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "chr_to_idx = {s:i+1 for i,s in enumerate(chars)}; print(chr_to_idx)\n",
    "chr_to_idx['.'] = 0\n",
    "idx_to_chr = {i:s for s,i in chr_to_idx.items()}; print(idx_to_chr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847a381",
   "metadata": {},
   "source": [
    "### Building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words: list, block_size: int = 3, verbose: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Take a list of words and creates Xs and Ys give\n",
    "        a block size. E.g. block_size=3 for emma would \n",
    "        result in:\n",
    "            ... ---> e  \n",
    "            ..e ---> m  \n",
    "            .em ---> m  \n",
    "            emm ---> a  \n",
    "            mma ---> . \n",
    "        Stored in a (n, 3) tensor\n",
    "    Inputs:\n",
    "        words: List of words\n",
    "        block_size: The context window\n",
    "        verbose: Where to print the outputs\n",
    "    Ouputs:\n",
    "        A (n, block_size) tensor with Xs\n",
    "        A (n) tensor with Ys\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        if verbose:\n",
    "            print(word)\n",
    "        context = [0] * block_size\n",
    "        for char in word + '.':\n",
    "            idx = chr_to_idx[char]\n",
    "            X.append(context)\n",
    "            Y.append(idx)\n",
    "            if verbose:\n",
    "                print(''.join(idx_to_chr[i] for i in context), idx_to_chr[idx])\n",
    "            context = context[1:] + [idx]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting names in random order\n",
    "random.shuffle(names)\n",
    "\n",
    "# Creating split indices\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ae2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building training, validation and test sets\n",
    "Xtr, Ytr = build_dataset(names[:n1]); print(Ytr.size())\n",
    "Xval, Yval = build_dataset(names[n1:n2]); print(Yval.size())\n",
    "Xtest, Ytest = build_dataset(names[n2:]); print(Ytest.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb38f44",
   "metadata": {},
   "source": [
    "# Network Building Blocks: Part I\n",
    "We are going to reuse the blocks we created in a previous lecture, and add a few extra, such that we can build the wavenet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0be090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        \"\"\" Initializes Weights \"\"\"\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\" Forward Pass \"\"\"\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\" Returning  All Parameters \"\"\"\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        \"\"\" Initializing Batchnorm Weights \"\"\"\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        \n",
    "        # Batchnorm Shift Parameters\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "        # Running batchnorm Mean and Variance\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\" Forward Pass and Buffers \"\"\"\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True, unbiased=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        \"\"\" Non-linearity \"\"\"\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f615f79",
   "metadata": {},
   "source": [
    "# Training a Network: Part I\n",
    "Here we build a simple network and train it. It will be this network that we continously expand on in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Weights\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "block_size = 3\n",
    "vocab_size = len(chr_to_idx)\n",
    "\n",
    "# Lookup table\n",
    "C = torch.randn([vocab_size, n_embd])\n",
    "\n",
    "# Layers\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "# Adjusing initialization of layers\n",
    "with torch.no_grad():\n",
    "    # Making last layer less confident\n",
    "    layers[-1].weight *= 0.1\n",
    "\n",
    "# All parameters\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985e01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lr = 0.1\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)    \n",
    "    \n",
    "    # Backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    lr = lr if step < 15000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * 0.01 * p.grad        \n",
    "        \n",
    "    # Tracking stats\n",
    "    if not step % 10000:\n",
    "        print(f\"{step:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d64476",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In the previous notebook we predicted the next character of a name only by looking at the previous character. Here we want to do something in a bit more sophistcated way, as we want to use more of the context than a single character, and we will be doing it in different ways: bag-of-words and using a multilayer perceptron (MLP). The MLP approach will be based on the a [paper from 2003](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbGJ0RndnaG1JbUhHVURROWVQVFNuOWZwU01oQXxBQ3Jtc0ttbW5SQnhQN0ZMWWtHem5FVFA5dFFCdk02R29raDVBNlMxaXpxU3E1S2dReHAxcVRYQjN3bjZsM2ZLcjdkRG5oWTBnSU1OUjZsaThxSnZLdVpKWEFWTDgzZnd3QlNmQXRldFFxRjZ1ekdfUFV0ZnVTcw&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I), in which they predict the next word using the previous words.\n",
    "\n",
    "We will be doing the following:\n",
    "\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9dc35",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e124022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6b656d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dbb64",
   "metadata": {},
   "source": [
    "# Data\n",
    "For creating the language models we use a dataset of the most common names from [ssa.gov](https://www.ssa.gov/oact/babynames/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698640b",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3d263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading names into a list\n",
    "with open('../../data/names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bc9da",
   "metadata": {},
   "source": [
    "### Creating Vocabulary\n",
    "As a neural network works with numbers, we need a way to translate back and forth between letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "891261be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary (character to/from index)\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "chr_to_idx = {s:i+1 for i,s in enumerate(chars)}; print(chr_to_idx)\n",
    "chr_to_idx['.'] = 0\n",
    "idx_to_chr = {i:s for s,i in chr_to_idx.items()}; print(idx_to_chr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d3685",
   "metadata": {},
   "source": [
    "### Preparing Dataset\n",
    "For each letter we will be using the previous X characters to predict it (block_size). \n",
    "\n",
    "Example for emma:  \n",
    "\n",
    "<pre>\n",
    "... ---> e  \n",
    "..e ---> m  \n",
    ".em ---> m  \n",
    "emm ---> a  \n",
    "mma ---> .  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9b7002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "verbose = False\n",
    "\n",
    "X, Y = [], []\n",
    "for name in names:\n",
    "    if verbose:\n",
    "        print(name)\n",
    "    context = [0] * block_size\n",
    "    for char in name + '.':\n",
    "        idx = chr_to_idx[char]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "        if verbose:\n",
    "            print(''.join(idx_to_chr[i] for i in context), idx_to_chr[idx])\n",
    "        context = context[1:] + [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89a9b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".em\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# Printing example x and y\n",
    "x2 = X[2]; print(''.join(idx_to_chr[x] for x in x2))\n",
    "y2 = Y[2]; print(idx_to_chr[y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3212168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to pytorch arrays\n",
    "X = torch.tensor(X) # n_examples x block_size\n",
    "Y = torch.tensor(Y) # n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa18e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples not to run out of memory\n",
    "n_samples = len(Y)\n",
    "X = X[:n_samples]\n",
    "Y = Y[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535f321",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "We now build the neural network as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b832643",
   "metadata": {},
   "source": [
    "### The Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ba0b",
   "metadata": {},
   "source": [
    "First we build an embedding lookup table. The lookup table is in concept similar to that of the one-hot encodings, as we in both cases represent the individual characters as vectors. One-hot vectors are the same length as the vocabulary, while embedding vectors can be arbitrarily short, depending on how much information you want them to be able to store.\n",
    "\n",
    "One-Hot Example with dictionary ABCD:\n",
    "<pre>\n",
    "  A B C D\n",
    "A 1 0 0 0\n",
    "B 0 1 0 0\n",
    "B 0 1 0 0\n",
    "A 1 0 0 0\n",
    "</pre>\n",
    "\n",
    "Lookup table Example with two dimenstions:\n",
    "<pre>\n",
    "Lookup table:\n",
    "    d1    d2\n",
    "A  0.1  -0.3\n",
    "B -0.5  -0.7\n",
    "C -0.1   1.3\n",
    "D  3.0   0.9\n",
    "\n",
    "Chars   Indicies     Embeddings\n",
    "ABBA -> [1,2,2,1] -> [[0.1, -0.3],[-0.5, -0.7],[-0.5, -0.7],[0.1, -0.3]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713f3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9934,  0.9937],\n",
      "        [ 0.9063,  0.1049],\n",
      "        [ 1.3440,  0.2482],\n",
      "        [-0.1903,  0.3603],\n",
      "        [ 0.3461, -0.0322],\n",
      "        [ 0.5782, -0.1122],\n",
      "        [-1.1620,  0.9323],\n",
      "        [ 1.7361, -0.1493],\n",
      "        [ 1.4606, -0.7879],\n",
      "        [-0.2157, -0.1100],\n",
      "        [ 0.3744,  0.2926],\n",
      "        [ 1.6140, -0.6878],\n",
      "        [ 1.7642,  0.5494],\n",
      "        [-1.3256,  1.5837],\n",
      "        [-0.1922,  0.0492],\n",
      "        [ 0.6667, -0.2482],\n",
      "        [-0.8593, -1.0975],\n",
      "        [-0.1792, -1.1473],\n",
      "        [ 0.0251,  0.0327],\n",
      "        [-0.1805, -0.6411],\n",
      "        [ 1.4852, -0.3194],\n",
      "        [ 1.4910,  0.5114],\n",
      "        [-2.3314, -1.7684],\n",
      "        [ 0.3082,  0.7290],\n",
      "        [ 1.6891, -1.2260],\n",
      "        [-0.0223,  1.6806],\n",
      "        [-1.1237,  0.1080]])\n"
     ]
    }
   ],
   "source": [
    "# Building a lookup table (vocab_length x n_dimensions)\n",
    "C = torch.randn([27, 2]); print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050aa2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: c\n",
      "Vocab Index: 3\n",
      "Embedding: tensor([-0.1903,  0.3603])\n"
     ]
    }
   ],
   "source": [
    "# Looking up the embedding of one character\n",
    "chr = \"c\"; print(f\"Character: {chr}\")\n",
    "idx = chr_to_idx[chr]; print(f\"Vocab Index: {idx}\")\n",
    "embedding = C[idx]; print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e42402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9063, 0.1049],\n",
       "        [1.3440, 0.2482],\n",
       "        [1.3440, 0.2482],\n",
       "        [0.9063, 0.1049]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking up embeddings of four characters: \"abba\"\n",
    "chars = torch.tensor([1,2,2,1])\n",
    "embeddings = C[chars]; embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc11f6f",
   "metadata": {},
   "source": [
    "It is also possible to make these lookups in higher dimensionality, e.g. in our X-data we created earlier we have two dimensions, rows (samples) and columns (context window). Here we will try to look all X data up in the C lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "696ab7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937]],\n",
       "\n",
       "        [[ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937],\n",
       "         [ 0.5782, -0.1122]],\n",
       "\n",
       "        [[ 0.9934,  0.9937],\n",
       "         [ 0.5782, -0.1122],\n",
       "         [-1.3256,  1.5837]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6667, -0.2482],\n",
       "         [ 1.7642,  0.5494],\n",
       "         [ 0.5782, -0.1122]],\n",
       "\n",
       "        [[ 1.7642,  0.5494],\n",
       "         [ 0.5782, -0.1122],\n",
       "         [-0.2157, -0.1100]],\n",
       "\n",
       "        [[ 0.5782, -0.1122],\n",
       "         [-0.2157, -0.1100],\n",
       "         [ 1.7642,  0.5494]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions are: n_samples x context_window x embedding_size\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50750e2",
   "metadata": {},
   "source": [
    "These were all examples, so what we bring from this section into the next part of the neural network is the **lookup table** as well as the **embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0768bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: vocab_size x embedding_dimension\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Embeddings\n",
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea596834",
   "metadata": {},
   "source": [
    "### Adding more layers to the network\n",
    "The different layers of the network fit together via their input- and output-dimensions. \n",
    "\n",
    "Examples of dimensions:  \n",
    "\n",
    "* samples: n_samples x context_window  \n",
    "* lookup table: vocab_size x embedding_size  \n",
    "* layer: (context_window * embedding_size) x n_neurons  \n",
    "\n",
    "After the samples have gone through the embedding layer we have a matrix pr sample of dimension context_window x embedding_size. Before we can multiply this output with the first neuron layer, we need to unstack it:\n",
    "\n",
    "<pre>\n",
    "[[1,2],\n",
    " [4,5],    ---> [1,2,3,4,5,6]\n",
    " [6,7]]\n",
    "</pre>\n",
    "\n",
    "Actually pytorch stores its data as a one-dimentional tensor all ready, and one can easily alter between the dimensionality using .view(). We will use that here to unstack the two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee87b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 1)\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6bba8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "# Unstack example\n",
    "e1 = torch.arange(18); print(e1)\n",
    "e2 = e1.view(3,3,2); print(e2)\n",
    "e3 = e2.view(3,6); print(e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698b180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unstacking sample dimensions and calculating activations\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 2)\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716039e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 27])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating logits for each possible output\n",
    "logits = h @ W2 + b2; logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cbd8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating probablitities pr character pr sample\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True); probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc84f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating loss\n",
    "### Without regularization\n",
    "### loss = -probs[:,ys[:n_samples]].log().mean()\n",
    "### With regulatization (Rewarding low Ws)\n",
    "loss = -probs[:,Y].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c247c3ce",
   "metadata": {},
   "source": [
    "Now we write it all into a few cells removing all the example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3c3950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Lookup table\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Weights and biases\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])\n",
    "print(\"Number of parameters: \" + str(sum(p.nelement() for p in [C, W1, b1, W2, b2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8533d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6651)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True); probs.size()\n",
    "loss = -probs[:,Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c8203",
   "metadata": {},
   "source": [
    "### Simplifying and Training Model\n",
    "All the places we can use the pytorch native metods we want to do that, e.g. pytorch can calculate the cross entropy loss for us instead of us doing it explicitely. Pytorch uses less memory, is more efficient and is better at handling extreme numbers. After simplifying we will write the code into a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df1c9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c5dda01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Lookup table\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Weights and biases\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])\n",
    "\n",
    "# Placing parameters in list\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "# Enabling gradients\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(\"Number of parameters: \" + str(sum(p.nelement() for p in parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12533fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0372, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.2221, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.4874, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.8202, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.2219, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.6884, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.2120, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7789, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0160, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.6726, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3471, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0378, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7447, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4682, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2084, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.9651, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7386, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5290, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3364, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1601, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9988, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8508, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7143, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5878, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4699, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3595, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2556, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1575, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0647, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9768, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8935, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8145, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7397, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6687, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6012, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5371, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4761, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4177, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3618, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3082, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2565, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2069, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1591, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1132, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0692, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0272, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9871, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9489, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9127, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8783, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8457, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8149, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7857, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7581, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7071, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6836, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6613, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6401, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6198, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6005, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5821, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5645, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5476, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5314, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5158, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4865, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4726, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4592, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4463, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4338, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4217, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4099, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3986, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3767, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3663, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3561, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3461, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3364, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3269, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3176, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3085, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2996, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2909, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2824, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2740, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2658, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2498, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2420, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2344, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2269, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2195, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     emb \u001b[38;5;241m=\u001b[39m C[X]\n\u001b[0;32m----> 4\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(emb\u001b[38;5;241m.\u001b[39mview(emb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], emb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39memb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1); h\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m      5\u001b[0m     logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y);\u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    # Forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y);print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7839ba",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "It can be quite heavy to process all the data in every forward/backward pass, which it why people often divided their data into mini-batches. This simply means to take a random group of data points for each epoch. This is introduced in this next training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db57a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4290, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9174, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0238, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9081, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7875, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6535, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9753, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2557, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9341, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4877, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2231, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5120, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6659, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6056, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1839, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3037, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9968, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4329, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0068, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8966, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3771, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3561, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4767, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3784, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5608, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9131, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2236, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0423, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1024, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7925, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9547, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3411, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5360, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2678, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1909, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9938, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9080, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8296, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8082, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5467, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3674, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0169, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9771, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2763, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8973, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9405, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2124, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5002, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8793, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2264, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1168, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0332, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0417, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9964, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0499, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4614, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7103, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2066, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8276, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3912, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2809, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0379, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7936, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0092, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1784, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8610, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1243, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0322, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1156, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7646, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7810, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5666, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6184, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9407, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7563, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9510, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9384, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2452, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2213, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8419, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5997, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2915, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0682, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8066, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0135, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8551, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6493, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7528, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6349, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9970, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3359, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6582, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9689, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4359, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8127, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1786, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3598, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8637, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7931, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6222, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9722, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5237, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1285, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9797, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0090, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3482, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5977, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8787, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1235, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3605, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3760, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0836, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1497, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2539, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0609, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8148, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3343, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0903, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7922, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7098, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8479, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1394, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9390, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8218, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7500, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1688, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8706, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9481, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5490, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7230, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7913, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4600, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1842, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8733, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1250, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8182, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9350, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3490, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2752, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1057, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0122, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0048, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1384, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2389, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0536, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8246, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2708, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0757, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3548, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4373, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9643, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6382, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5844, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8389, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8067, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9457, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9107, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9621, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0762, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9071, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1448, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6818, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8252, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5536, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7247, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7850, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0794, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7904, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9287, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3599, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8511, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0442, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1225, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2533, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8447, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1884, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1626, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8536, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1486, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9785, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9731, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8550, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4805, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2133, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4725, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7615, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8700, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3567, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0637, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8362, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1884, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9570, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9875, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7466, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1167, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1855, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5172, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7287, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4231, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8115, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2807, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6318, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4165, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8839, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2518, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7628, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8442, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9572, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2338, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8606, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8109, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5723, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9160, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0492, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1466, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0796, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8513, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8291, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7632, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9241, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3833, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5573, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7532, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5423, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5810, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9773, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8136, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5791, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1583, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9032, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5232, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6782, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9476, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9089, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1760, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7357, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8617, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8816, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3654, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8337, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7845, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7816, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8625, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8914, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6297, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0022, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9820, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4952, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4045, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7277, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8636, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6125, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4341, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0902, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6432, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8214, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1450, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9563, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7930, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5463, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5800, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6323, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7783, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5462, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9333, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4687, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0066, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9965, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8067, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8621, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0550, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8339, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6854, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9681, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6531, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8256, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4772, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7976, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6248, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8716, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7360, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2900, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8335, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7263, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9187, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7327, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8494, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8919, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9216, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3499, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9252, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9072, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0002, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5674, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0292, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9487, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5917, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9424, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8502, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4244, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1980, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6474, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6286, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3584, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7741, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0921, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7480, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4561, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8638, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6382, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7235, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9696, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2705, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7161, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7276, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6765, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3449, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4431, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8763, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6537, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7976, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3837, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5706, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8599, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5113, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9851, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8672, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8690, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6316, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8172, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6551, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1871, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0315, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0660, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7452, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5752, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4884, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6400, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5052, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3906, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9007, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4466, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7611, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5790, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8650, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0736, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6510, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7346, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5830, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8515, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9566, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3642, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7784, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9585, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7175, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2089, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6224, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4986, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9882, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7324, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0282, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8107, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6014, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5311, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0400, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9911, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7102, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6477, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3660, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5923, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8250, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7338, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5515, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0027, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8535, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4554, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9243, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7927, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0596, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4475, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3611, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9686, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7684, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9882, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7036, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5279, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9833, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7392, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6697, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7171, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7331, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4253, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6914, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8587, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5916, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6858, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4703, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7633, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9849, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6082, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8002, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0132, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6436, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6377, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8842, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7496, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3320, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5832, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5182, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4532, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5218, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7605, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1579, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1835, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8598, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5449, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3728, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5385, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7469, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7476, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1650, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7104, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5639, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8562, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1178, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5822, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4472, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8123, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8214, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5696, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1317, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7930, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4132, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8765, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8578, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4917, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7195, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2440, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2830, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8179, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2333, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1505, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3760, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3922, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5213, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5512, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3884, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9111, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1850, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3388, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6710, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7621, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5304, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6325, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9843, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2798, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9091, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5350, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5501, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4732, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9559, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8808, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3120, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6775, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6167, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9804, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7090, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5398, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3522, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7591, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9730, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6259, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3358, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0087, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2249, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0194, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7186, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4385, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4162, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3571, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3377, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9249, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7832, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2487, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8829, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7650, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5342, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2826, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4543, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9895, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4929, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9695, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2612, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1457, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5805, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2820, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6129, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1092, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9441, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9526, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6680, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8253, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4926, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4756, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3508, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3317, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4232, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4942, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6415, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8348, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8761, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6614, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6088, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5451, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7199, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5197, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8295, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5412, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1655, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5738, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5486, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5349, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5923, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6477, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3945, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6442, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1755, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7866, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6260, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9191, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4222, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3811, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2937, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7863, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8406, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8751, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9153, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3500, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6608, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7143, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1306, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4296, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5990, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8177, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1492, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7497, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7513, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6705, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0630, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8952, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9208, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7264, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4717, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6281, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5545, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6951, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3789, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4508, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3830, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1347, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2739, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2344, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8076, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8674, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8816, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5913, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5033, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7692, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2488, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0004, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6796, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6281, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6877, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8105, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6974, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5602, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8032, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5885, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7496, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5744, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8433, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4635, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5367, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7106, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2946, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6675, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5816, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6984, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9294, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5933, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3483, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7548, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7500, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6927, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9376, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4267, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5744, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9553, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5257, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3628, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5581, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6966, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6538, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2152, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7399, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8159, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8192, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9215, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6786, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8168, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7378, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2779, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6246, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5435, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0228, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6506, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5016, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2448, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3657, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3817, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7597, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7913, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6089, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5868, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0669, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8344, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5424, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5483, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9167, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7545, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5991, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7217, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9481, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2933, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4653, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6455, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4803, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7050, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7128, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6908, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2572, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8294, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7234, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6854, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6549, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8393, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6478, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4642, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8197, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6426, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5062, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8155, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1301, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5109, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5434, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5225, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6397, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7425, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3991, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4529, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1094, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5685, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4279, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1018, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6314, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5237, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8466, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7324, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5747, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3222, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3346, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7997, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5689, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3299, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6742, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3899, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6104, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9381, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3666, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3795, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4621, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4967, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9655, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5548, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7558, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4474, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9626, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1176, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4676, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8443, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2994, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9935, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3325, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8487, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0602, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5739, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6865, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0081, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5606, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9880, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9180, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4495, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7269, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5505, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4455, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4562, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6877, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0415, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8775, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4639, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6094, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6122, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2641, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6968, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5039, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6048, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8026, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5084, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7206, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6285, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4697, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8717, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4534, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6981, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1052, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7570, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7629, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8042, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6964, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6843, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0546, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8543, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7518, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5647, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6873, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6200, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6371, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8584, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6804, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0725, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7328, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1418, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6607, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4743, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2632, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5130, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2316, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6012, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5381, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4037, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7668, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9468, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7442, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9313, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5305, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9077, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8954, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8849, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5988, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6032, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4177, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7441, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6453, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7817, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    # Minibatch (32 random sample indices/integers)\n",
    "    ix = torch.randint(0, X.shape[0], (minibatch_size,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix]);print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "52a6c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6426, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Full loss\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y);print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b0dca",
   "metadata": {},
   "source": [
    "The gradient with minibatches will be much less stable/precise than with all the data, but it is still better to use minibatches with many epochs than to run few epochs with all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99efef2",
   "metadata": {},
   "source": [
    "### What is a resonable learning rate?\n",
    "Here we will try to identify the best learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4aeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

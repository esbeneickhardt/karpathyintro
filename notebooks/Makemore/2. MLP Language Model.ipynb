{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d64476",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In the previous notebook we predicted the next character of a name only by looking at the previous character. Here we want to do something in a bit more sophistcated way, as we want to use more of the context than a single character, and we will be doing it in different ways: bag-of-words and using a multilayer perceptron (MLP). The MLP approach will be based on the a [paper from 2003](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbGJ0RndnaG1JbUhHVURROWVQVFNuOWZwU01oQXxBQ3Jtc0ttbW5SQnhQN0ZMWWtHem5FVFA5dFFCdk02R29raDVBNlMxaXpxU3E1S2dReHAxcVRYQjN3bjZsM2ZLcjdkRG5oWTBnSU1OUjZsaThxSnZLdVpKWEFWTDgzZnd3QlNmQXRldFFxRjZ1ekdfUFV0ZnVTcw&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I), in which they predict the next word using the previous words.\n",
    "\n",
    "We will be doing the following:\n",
    "\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9dc35",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e124022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6b656d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dbb64",
   "metadata": {},
   "source": [
    "# Data\n",
    "For creating the language models we use a dataset of the most common names from [ssa.gov](https://www.ssa.gov/oact/babynames/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698640b",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3d263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading names into a list\n",
    "with open('../../data/names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bc9da",
   "metadata": {},
   "source": [
    "### Creating Vocabulary\n",
    "As a neural network works with numbers, we need a way to translate back and forth between letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "891261be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary (character to/from index)\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "chr_to_idx = {s:i+1 for i,s in enumerate(chars)}; print(chr_to_idx)\n",
    "chr_to_idx['.'] = 0\n",
    "idx_to_chr = {i:s for s,i in chr_to_idx.items()}; print(idx_to_chr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d3685",
   "metadata": {},
   "source": [
    "### Preparing Dataset\n",
    "For each letter we will be using the previous X characters to predict it (block_size). \n",
    "\n",
    "Example for emma:  \n",
    "\n",
    "<pre>\n",
    "... ---> e  \n",
    "..e ---> m  \n",
    ".em ---> m  \n",
    "emm ---> a  \n",
    "mma ---> .  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e9b7002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "verbose = False\n",
    "\n",
    "X, Y = [], []\n",
    "for name in names:\n",
    "    if verbose:\n",
    "        print(name)\n",
    "    context = [0] * block_size\n",
    "    for char in name + '.':\n",
    "        idx = chr_to_idx[char]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "        if verbose:\n",
    "            print(''.join(idx_to_chr[i] for i in context), idx_to_chr[idx])\n",
    "        context = context[1:] + [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89a9b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".em\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# Printing example x and y\n",
    "x2 = X[2]; print(''.join(idx_to_chr[x] for x in x2))\n",
    "y2 = Y[2]; print(idx_to_chr[y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3212168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to pytorch arrays\n",
    "X = torch.tensor(X) # n_examples x block_size\n",
    "Y = torch.tensor(Y) # n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74f17514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples not to run out of memory\n",
    "n_samples = len(Y)\n",
    "X = X[:n_samples]\n",
    "Y = Y[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535f321",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "We now build the neural network as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b832643",
   "metadata": {},
   "source": [
    "### The Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ba0b",
   "metadata": {},
   "source": [
    "First we build an embedding lookup table. The lookup table is in concept similar to that of the one-hot encodings, as we in both cases represent the individual characters as vectors. One-hot vectors are the same length as the vocabulary, while embedding vectors can be arbitrarily short, depending on how much information you want them to be able to store.\n",
    "\n",
    "One-Hot Example with dictionary ABCD:\n",
    "<pre>\n",
    "  A B C D\n",
    "A 1 0 0 0\n",
    "B 0 1 0 0\n",
    "B 0 1 0 0\n",
    "A 1 0 0 0\n",
    "</pre>\n",
    "\n",
    "Lookup table Example with two dimenstions:\n",
    "<pre>\n",
    "Lookup table:\n",
    "    d1    d2\n",
    "A  0.1  -0.3\n",
    "B -0.5  -0.7\n",
    "C -0.1   1.3\n",
    "D  3.0   0.9\n",
    "\n",
    "Chars   Indicies     Embeddings\n",
    "ABBA -> [1,2,2,1] -> [[0.1, -0.3],[-0.5, -0.7],[-0.5, -0.7],[0.1, -0.3]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713f3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9934,  0.9937],\n",
      "        [ 0.9063,  0.1049],\n",
      "        [ 1.3440,  0.2482],\n",
      "        [-0.1903,  0.3603],\n",
      "        [ 0.3461, -0.0322],\n",
      "        [ 0.5782, -0.1122],\n",
      "        [-1.1620,  0.9323],\n",
      "        [ 1.7361, -0.1493],\n",
      "        [ 1.4606, -0.7879],\n",
      "        [-0.2157, -0.1100],\n",
      "        [ 0.3744,  0.2926],\n",
      "        [ 1.6140, -0.6878],\n",
      "        [ 1.7642,  0.5494],\n",
      "        [-1.3256,  1.5837],\n",
      "        [-0.1922,  0.0492],\n",
      "        [ 0.6667, -0.2482],\n",
      "        [-0.8593, -1.0975],\n",
      "        [-0.1792, -1.1473],\n",
      "        [ 0.0251,  0.0327],\n",
      "        [-0.1805, -0.6411],\n",
      "        [ 1.4852, -0.3194],\n",
      "        [ 1.4910,  0.5114],\n",
      "        [-2.3314, -1.7684],\n",
      "        [ 0.3082,  0.7290],\n",
      "        [ 1.6891, -1.2260],\n",
      "        [-0.0223,  1.6806],\n",
      "        [-1.1237,  0.1080]])\n"
     ]
    }
   ],
   "source": [
    "# Building a lookup table (vocab_length x n_dimensions)\n",
    "C = torch.randn([27, 2]); print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050aa2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: c\n",
      "Vocab Index: 3\n",
      "Embedding: tensor([-0.1903,  0.3603])\n"
     ]
    }
   ],
   "source": [
    "# Looking up the embedding of one character\n",
    "chr = \"c\"; print(f\"Character: {chr}\")\n",
    "idx = chr_to_idx[chr]; print(f\"Vocab Index: {idx}\")\n",
    "embedding = C[idx]; print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e42402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9063, 0.1049],\n",
       "        [1.3440, 0.2482],\n",
       "        [1.3440, 0.2482],\n",
       "        [0.9063, 0.1049]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking up embeddings of four characters: \"abba\"\n",
    "chars = torch.tensor([1,2,2,1])\n",
    "embeddings = C[chars]; embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc11f6f",
   "metadata": {},
   "source": [
    "It is also possible to make these lookups in higher dimensionality, e.g. in our X-data we created earlier we have two dimensions, rows (samples) and columns (context window). Here we will try to look all X data up in the C lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "696ab7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937]],\n",
       "\n",
       "        [[ 0.9934,  0.9937],\n",
       "         [ 0.9934,  0.9937],\n",
       "         [ 0.5782, -0.1122]],\n",
       "\n",
       "        [[ 0.9934,  0.9937],\n",
       "         [ 0.5782, -0.1122],\n",
       "         [-1.3256,  1.5837]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.6667, -0.2482],\n",
       "         [ 1.7642,  0.5494],\n",
       "         [ 0.5782, -0.1122]],\n",
       "\n",
       "        [[ 1.7642,  0.5494],\n",
       "         [ 0.5782, -0.1122],\n",
       "         [-0.2157, -0.1100]],\n",
       "\n",
       "        [[ 0.5782, -0.1122],\n",
       "         [-0.2157, -0.1100],\n",
       "         [ 1.7642,  0.5494]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions are: n_samples x context_window x embedding_size\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50750e2",
   "metadata": {},
   "source": [
    "These were all examples, so what we bring from this section into the next part of the neural network is the **lookup table** as well as the **embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0768bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: vocab_size x embedding_dimension\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Embeddings\n",
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea596834",
   "metadata": {},
   "source": [
    "### Adding more layers to the network\n",
    "The different layers of the network fit together via their input- and output-dimensions. \n",
    "\n",
    "Examples of dimensions:  \n",
    "\n",
    "* samples: n_samples x context_window  \n",
    "* lookup table: vocab_size x embedding_size  \n",
    "* layer: (context_window * embedding_size) x n_neurons  \n",
    "\n",
    "After the samples have gone through the embedding layer we have a matrix pr sample of dimension context_window x embedding_size. Before we can multiply this output with the first neuron layer, we need to unstack it:\n",
    "\n",
    "<pre>\n",
    "[[1,2],\n",
    " [4,5],    ---> [1,2,3,4,5,6]\n",
    " [6,7]]\n",
    "</pre>\n",
    "\n",
    "Actually pytorch stores its data as a one-dimentional tensor all ready, and one can easily alter between the dimensionality using .view(). We will use that here to unstack the two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee87b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 1)\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6bba8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "# Unstack example\n",
    "e1 = torch.arange(18); print(e1)\n",
    "e2 = e1.view(3,3,2); print(e2)\n",
    "e3 = e2.view(3,6); print(e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698b180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 100])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unstacking sample dimensions and calculating activations\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 2)\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "716039e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 27])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating logits for each possible output\n",
    "logits = h @ W2 + b2; logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cbd8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating probablitities pr character pr sample\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True); probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8138e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating loss\n",
    "### Without regularization\n",
    "### loss = -probs[:,ys[:n_samples]].log().mean()\n",
    "### With regulatization (Rewarding low Ws)\n",
    "loss = -probs[:,Y].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefc35d",
   "metadata": {},
   "source": [
    "Now we write it all into a few cells removing all the example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d61febd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Lookup table\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Weights and biases\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])\n",
    "print(\"Number of parameters: \" + str(sum(p.nelement() for p in [C, W1, b1, W2, b2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "211c1f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.6651)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True); probs.size()\n",
    "loss = -probs[:,Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127fc0c",
   "metadata": {},
   "source": [
    "### Simplifying and Training Model\n",
    "All the places we can use the pytorch native metods we want to do that, e.g. pytorch can calculate the cross entropy loss for us instead of us doing it explicitely. Pytorch uses less memory, is more efficient and is better at handling extreme numbers. After simplifying we will write the code into a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e440844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 1000\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3cf97571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3481\n"
     ]
    }
   ],
   "source": [
    "# Lookup table\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Weights and biases\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])\n",
    "\n",
    "# Placing parameters in list\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "# Enabling gradients\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "print(\"Number of parameters: \" + str(sum(p.nelement() for p in parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "008b3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.0372, grad_fn=<NllLossBackward0>)\n",
      "tensor(13.2221, grad_fn=<NllLossBackward0>)\n",
      "tensor(12.4874, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.8202, grad_fn=<NllLossBackward0>)\n",
      "tensor(11.2219, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.6884, grad_fn=<NllLossBackward0>)\n",
      "tensor(10.2120, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7789, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.3854, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0160, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.6726, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.3471, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0378, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7447, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4682, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2084, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.9651, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.7386, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.5290, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3364, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1601, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9988, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8508, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7143, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5878, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4699, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3595, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2556, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1575, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0647, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9768, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8935, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8145, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7397, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6687, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6012, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5371, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4761, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4177, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3618, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3082, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2565, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2069, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1591, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1132, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0692, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0272, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9871, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9489, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9127, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8783, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8457, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8149, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7857, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7581, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7071, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6836, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6613, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6401, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6198, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6005, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5821, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5645, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5476, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5314, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5158, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5008, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4865, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4726, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4592, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4463, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4338, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4217, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4099, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3986, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3875, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3767, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3663, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3561, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3461, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3364, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3269, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3176, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3085, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2996, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2909, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2824, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2740, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2658, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2498, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2420, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2344, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2269, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2195, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     emb \u001b[38;5;241m=\u001b[39m C[X]\n\u001b[0;32m----> 4\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(emb\u001b[38;5;241m.\u001b[39mview(emb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], emb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39memb\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1); h\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m      5\u001b[0m     logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, Y);\u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    # Forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y);print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d75784",
   "metadata": {},
   "source": [
    "### Mini-batching\n",
    "It can be quite heavy to process all the data in every forward/backward pass, which it why people often divided their data into mini-batches. This simply means to take a random group of data points for each epoch. This is introduced in this next training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a614cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4290, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9174, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0238, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9081, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7875, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6535, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9753, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2557, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9341, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4877, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2231, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5120, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6659, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6056, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1839, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3037, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9968, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4329, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0068, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8966, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3771, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3561, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4767, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3784, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5608, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9131, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2236, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0423, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1024, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7925, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9547, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3411, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5360, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2678, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1909, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9938, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9080, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8296, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8082, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5467, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3674, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0169, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9771, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2763, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8973, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9405, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2124, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5002, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8793, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2264, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1168, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0332, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0417, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9964, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0499, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4614, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7103, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2066, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8276, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3912, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2809, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0379, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7936, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0092, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1784, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8610, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1243, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0322, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1156, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7646, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7810, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5666, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6184, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3864, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9407, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7563, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9510, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9384, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2452, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2213, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8419, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5997, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2915, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0682, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8066, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0135, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8551, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6493, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7528, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6349, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9970, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3359, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6582, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9689, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4359, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8127, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1786, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3598, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8637, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7931, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6222, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9722, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5237, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1285, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9797, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0090, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3482, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5977, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8787, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1235, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3605, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3760, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0836, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1497, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2539, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0609, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8148, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3343, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0903, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7922, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7098, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8479, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1394, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9390, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8218, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7500, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1688, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8706, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9481, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5490, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7230, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7913, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4600, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1842, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8733, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1250, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8182, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9350, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3490, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2752, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1057, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0122, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0048, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1384, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2389, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0536, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8246, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2708, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0757, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3548, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4373, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9643, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6041, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6382, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5844, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8389, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8067, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9457, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9107, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9621, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0762, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9071, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1448, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6818, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8252, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5536, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7247, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7850, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0794, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7904, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9287, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3599, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8511, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0442, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1225, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2533, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8447, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1884, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1626, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8536, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1486, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9785, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9731, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8550, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4805, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2133, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4725, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7615, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8700, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3567, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0637, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8362, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1884, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9570, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9875, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7466, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1167, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1855, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5172, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7287, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4231, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8115, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2807, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6318, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4165, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8839, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2518, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7628, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8442, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9572, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2338, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8606, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8109, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5723, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9160, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0492, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1466, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0796, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8513, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8291, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7632, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9241, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3833, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5573, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7532, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5423, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5810, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9773, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8136, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5791, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1583, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9032, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5232, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6782, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9476, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9089, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1760, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7357, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8617, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8816, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3654, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8337, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7845, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7816, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8625, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8914, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6297, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0022, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9820, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4952, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4045, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7277, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8636, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6125, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4341, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0902, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6432, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8214, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1450, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9563, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7930, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5463, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5800, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6323, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7783, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5462, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2170, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9333, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4687, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0066, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9965, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8067, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8621, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0550, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8339, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6854, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9681, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6531, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8256, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4772, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7976, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6248, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8716, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7360, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2900, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8335, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4126, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7263, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9187, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7327, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8494, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8919, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9216, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3499, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9252, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9072, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0002, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5674, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0292, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9487, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5917, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9424, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8502, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4244, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1980, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6474, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6286, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3584, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7741, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0921, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7480, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4561, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8638, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6382, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7235, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7981, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9696, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2705, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7161, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7276, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6765, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3449, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4431, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8763, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6537, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7976, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3837, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5706, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8599, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5113, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9851, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8672, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8690, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6316, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8172, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6551, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1871, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0315, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0660, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7452, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5752, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8193, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4884, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6400, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5052, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3906, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9007, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4466, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7611, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5790, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8650, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0736, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6510, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7346, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5830, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8515, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9566, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3642, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7784, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9585, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7175, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2089, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7890, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6224, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4986, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9882, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7324, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0282, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8107, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6014, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5311, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0400, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9911, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7102, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6477, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3660, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5923, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8250, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7338, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8139, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5515, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0027, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8535, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4554, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9243, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7927, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0596, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4475, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3611, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9686, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7684, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9586, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9882, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7036, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5279, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9833, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7392, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6697, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7171, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7331, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4253, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6914, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8587, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5916, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6858, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4703, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7633, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9849, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6082, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8002, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0132, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6436, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5162, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6377, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8842, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7496, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3320, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5832, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5182, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4532, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5218, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5063, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7605, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1579, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1835, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8598, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5449, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3728, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5385, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7469, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7476, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1650, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6328, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7104, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5639, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8562, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1178, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0240, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5822, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6108, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4472, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8123, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8214, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5696, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1317, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7930, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4132, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8765, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8578, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4917, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7195, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2440, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2830, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8179, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2333, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1505, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3760, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3922, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5213, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5512, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4207, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3884, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9111, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1850, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3388, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6710, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7621, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5304, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6325, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9843, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2798, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9091, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5350, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5501, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4732, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9559, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8808, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3120, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6775, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6167, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9804, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7090, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5398, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3522, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7591, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9730, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6259, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3358, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0087, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2249, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0194, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7186, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8272, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4385, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4162, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3571, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3377, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9249, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7832, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2487, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8829, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7650, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5342, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2826, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4543, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9895, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4929, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9695, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2612, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1457, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5805, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2820, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6129, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1092, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9441, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9526, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6869, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5687, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6680, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7015, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8253, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4926, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4756, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3508, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3317, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8595, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8236, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4232, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6883, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4942, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6415, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8348, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8761, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6614, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6088, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5451, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7199, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5197, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8295, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5412, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1655, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5738, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5486, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5349, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5923, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6477, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3945, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6442, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1755, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4975, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4121, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2992, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6852, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7866, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6260, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2031, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9191, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4222, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7934, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3811, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2937, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7863, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8406, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8751, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9153, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3500, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5471, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6608, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7143, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1306, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4296, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5990, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8177, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1492, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7497, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7513, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6705, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0630, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8952, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7970, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9208, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7264, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4717, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6281, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8221, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5545, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6951, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3789, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4508, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3830, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1347, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2739, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4202, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2344, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8076, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8674, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8816, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1714, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5913, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5033, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3075, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5044, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7692, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2488, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0004, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6796, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6281, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9943, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6877, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8105, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6974, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5602, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8032, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5885, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7496, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5744, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8433, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7749, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4635, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5367, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7106, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7112, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2946, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7332, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6675, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5816, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6984, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9294, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5933, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3483, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7548, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7500, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6927, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9376, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4267, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6519, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5744, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8868, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9553, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5257, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3628, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3110, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5581, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6966, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6538, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2152, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7399, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8159, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8192, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3886, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8879, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9215, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5099, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6786, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8168, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7378, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2779, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6246, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5435, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0577, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0228, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6506, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5016, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2448, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3657, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3817, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7597, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7913, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6089, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5868, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0669, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8344, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5424, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5483, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9167, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7545, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5251, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5991, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7217, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9481, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8683, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2933, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4653, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6455, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4803, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7083, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7050, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9819, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7128, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6908, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2572, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8294, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7234, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6854, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6549, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8616, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8393, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6478, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4642, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8197, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6426, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5062, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8155, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1301, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5109, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5434, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5225, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6397, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7425, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3991, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4529, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1094, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5685, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4279, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1018, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6314, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5237, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8135, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8466, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7324, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5747, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3222, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3346, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7997, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5689, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3299, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6742, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3899, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6104, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9381, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3666, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3795, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4621, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4967, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9655, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2087, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5548, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7558, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4474, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9626, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1176, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4676, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8443, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2994, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9935, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3325, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8487, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0602, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5739, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6865, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0081, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6001, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5606, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9880, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9180, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4495, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7269, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5505, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4455, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4562, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5227, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6877, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0415, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8775, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4639, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6094, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6122, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2641, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6968, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5039, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6048, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4696, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8026, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5084, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7206, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6285, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9068, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4697, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8717, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4534, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6047, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8178, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6981, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0324, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1052, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7570, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7629, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6030, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8042, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6893, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6964, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6843, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0546, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8543, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7518, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5647, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4983, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6873, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6623, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6200, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6371, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8584, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6804, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0725, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3652, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7328, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1418, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6607, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7319, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4743, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2632, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5130, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2316, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1908, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6012, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5381, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4037, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7668, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9468, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7010, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7442, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5005, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9313, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5305, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9077, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9881, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8954, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8849, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5988, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6032, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4177, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7441, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6453, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2552, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7679, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5995, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3086, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7817, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epochs):\n",
    "    # Minibatch (32 random sample indices/integers)\n",
    "    ix = torch.randint(0, X.shape[0], (minibatch_size,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix]);print(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "39cdd9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6426, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Full loss\n",
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y);print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247f059",
   "metadata": {},
   "source": [
    "The gradient with minibatches will be much less stable/precise than with all the data, but it is still better to use minibatches with many epochs than to run few epochs with all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40df6fe",
   "metadata": {},
   "source": [
    "### What is a resonable learning rate?\n",
    "Here we will try to identify the best learning rate by searching over a bunch of learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1c7f706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0010, 0.0010, 0.0010])\n",
      "tensor([0.9863, 0.9931, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Learning rate exponent candidates\n",
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre; print(lrs[:3]); print(lrs[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d0a38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing learning rates\n",
    "lri = []\n",
    "lossi = []\n",
    "for i in range(1000):\n",
    "    # Minibatch (32 random sample indices/integers)\n",
    "    ix = torch.randint(0, X.shape[0], (minibatch_size,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    ## Update\n",
    "    lr = lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # Tracking stats\n",
    "    lri.append(lr)\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b76151b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7a0daecdf0>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting learning rates vs losses\n",
    "plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b828df6",
   "metadata": {},
   "source": [
    "Now we can see that the best learning rate is the best when it is around 0.01. It is, however, just a rought estimate, and it likely that the learning rate should e.g. vary between layers, and between iterations. E.g. for the last few iterations one would use a lower learning rate (learning rate decay). This will, however, not be covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue: http://www.youtube.com/watch?v=TCH_1BHY58I&t=52m20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

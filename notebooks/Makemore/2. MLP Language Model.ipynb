{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d64476",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In the previous notebook we predicted the next character of a name only by looking at the previous character. Here we want to do something a bit more sophistcated, we want to use more of the context than a single character, and we will be doing it in different ways: bag-of-words and using a multilayer perceptron (MLP). The MLP approach will be based on the a [paper from 2003](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbGJ0RndnaG1JbUhHVURROWVQVFNuOWZwU01oQXxBQ3Jtc0ttbW5SQnhQN0ZMWWtHem5FVFA5dFFCdk02R29raDVBNlMxaXpxU3E1S2dReHAxcVRYQjN3bjZsM2ZLcjdkRG5oWTBnSU1OUjZsaThxSnZLdVpKWEFWTDgzZnd3QlNmQXRldFFxRjZ1ekdfUFV0ZnVTcw&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=TCH_1BHY58I), in which they predict the next word using the previous words.\n",
    "\n",
    "We will be doing the following:\n",
    "\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9dc35",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e124022",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b656d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dbb64",
   "metadata": {},
   "source": [
    "# Data\n",
    "For creating the language models we use a dataset of the most common names from [ssa.gov](https://www.ssa.gov/oact/babynames/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698640b",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d263a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading names into a list\n",
    "with open('../../data/names.txt', 'r') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bc9da",
   "metadata": {},
   "source": [
    "### Creating Vocabulary\n",
    "As a neural network works with numbers, we need a way to translate back and forth between letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891261be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary (character to/from index)\n",
    "chars = sorted(list(set(''.join(names))))\n",
    "chr_to_idx = {s:i+1 for i,s in enumerate(chars)}; print(chr_to_idx)\n",
    "chr_to_idx['.'] = 0\n",
    "idx_to_chr = {i:s for s,i in chr_to_idx.items()}; print(idx_to_chr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d3685",
   "metadata": {},
   "source": [
    "### Preparing Dataset\n",
    "For each letter we will be using the previous X characters to predict it (block_size). \n",
    "\n",
    "Example for emma:  \n",
    "\n",
    "<pre>\n",
    "... ---> e  \n",
    "..e ---> m  \n",
    ".em ---> m  \n",
    "emm ---> a  \n",
    "mma ---> .  \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b7002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "verbose = False\n",
    "\n",
    "X, Y = [], []\n",
    "for name in names:\n",
    "    if verbose:\n",
    "        print(name)\n",
    "    context = [0] * block_size\n",
    "    for char in name + '.':\n",
    "        idx = chr_to_idx[char]\n",
    "        X.append(context)\n",
    "        Y.append(idx)\n",
    "        if verbose:\n",
    "            print(''.join(idx_to_chr[i] for i in context), idx_to_chr[idx])\n",
    "        context = context[1:] + [idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a9b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".em\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "# Printing example x and y\n",
    "x2 = X[2]; print(''.join(idx_to_chr[x] for x in x2))\n",
    "y2 = Y[2]; print(idx_to_chr[y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3212168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to pytorch arrays\n",
    "X = torch.tensor(X) # n_examples x block_size\n",
    "Y = torch.tensor(Y) # n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2535f321",
   "metadata": {},
   "source": [
    "# Building the Neural Network\n",
    "We now build the neural network as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b832643",
   "metadata": {},
   "source": [
    "### The Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ba0b",
   "metadata": {},
   "source": [
    "First we build an embedding lookup table. The lookup table is in concept similar to that of the one-hot encodings, as we in both cases represent the individual characters as vectors. One-hot vectors are the same length as the vocabulary, while embedding vectors can be arbitrarily short, depending on how much information you want them to be able to store.\n",
    "\n",
    "One-Hot Example with dictionary ABCD:\n",
    "<pre>\n",
    "  A B C D\n",
    "A 1 0 0 0\n",
    "B 0 1 0 0\n",
    "B 0 1 0 0\n",
    "A 1 0 0 0\n",
    "</pre>\n",
    "\n",
    "Lookup table Example with two dimenstions:\n",
    "<pre>\n",
    "Lookup table:\n",
    "    d1    d2\n",
    "A  0.1  -0.3\n",
    "B -0.5  -0.7\n",
    "C -0.1   1.3\n",
    "D  3.0   0.9\n",
    "\n",
    "Chars   Indicies     Embeddings\n",
    "ABBA -> [1,2,2,1] -> [[0.1, -0.3],[-0.5, -0.7],[-0.5, -0.7],[0.1, -0.3]]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713f3eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0060,  1.5929],\n",
      "        [ 0.7376, -1.8402],\n",
      "        [ 1.8019,  0.6182],\n",
      "        [-1.2913, -0.5859],\n",
      "        [-1.5063,  1.3944],\n",
      "        [-0.1463,  0.2447],\n",
      "        [ 0.0365, -0.6914],\n",
      "        [-1.0483,  0.3444],\n",
      "        [-2.4608,  0.2352],\n",
      "        [ 0.4250, -0.6164],\n",
      "        [-1.5512,  1.1415],\n",
      "        [ 0.0527, -0.3730],\n",
      "        [ 0.1498, -0.6145],\n",
      "        [-0.0960, -1.7444],\n",
      "        [-0.1050, -0.3841],\n",
      "        [ 0.2512, -1.3238],\n",
      "        [ 0.7428,  0.6579],\n",
      "        [ 0.5280, -2.4567],\n",
      "        [-0.4924, -0.8601],\n",
      "        [ 0.5244,  0.3574],\n",
      "        [ 0.1042, -1.3714],\n",
      "        [-0.6479,  0.6436],\n",
      "        [-0.3895, -0.5352],\n",
      "        [-0.2469, -0.8324],\n",
      "        [-0.4192, -0.4529],\n",
      "        [ 0.5424,  1.5989],\n",
      "        [-1.1832, -0.1410]])\n"
     ]
    }
   ],
   "source": [
    "# Building a lookup table (vocab_length x n_dimensions)\n",
    "C = torch.randn([27, 2]); print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050aa2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: c\n",
      "Vocab Index: 3\n",
      "Embedding: tensor([-1.2913, -0.5859])\n"
     ]
    }
   ],
   "source": [
    "# Looking up the embedding of one character\n",
    "chr = \"c\"; print(f\"Character: {chr}\")\n",
    "idx = chr_to_idx[chr]; print(f\"Vocab Index: {idx}\")\n",
    "embedding = C[idx]; print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e42402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7376, -1.8402],\n",
       "        [ 1.8019,  0.6182],\n",
       "        [ 1.8019,  0.6182],\n",
       "        [ 0.7376, -1.8402]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking up embeddings of four characters: \"abba\"\n",
    "chars = torch.tensor([1,2,2,1])\n",
    "embeddings = C[chars]; embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc11f6f",
   "metadata": {},
   "source": [
    "It is also possible to make these lookups in higher dimensionality, e.g. in our X-data we created earlier we have two dimensions, rows (samples) and columns (context window). Here we will try to look all X data up in the C lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696ab7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0060,  1.5929],\n",
       "         [ 0.0060,  1.5929],\n",
       "         [ 0.0060,  1.5929]],\n",
       "\n",
       "        [[ 0.0060,  1.5929],\n",
       "         [ 0.0060,  1.5929],\n",
       "         [-0.1463,  0.2447]],\n",
       "\n",
       "        [[ 0.0060,  1.5929],\n",
       "         [-0.1463,  0.2447],\n",
       "         [-0.0960, -1.7444]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.1832, -0.1410],\n",
       "         [-1.1832, -0.1410],\n",
       "         [ 0.5424,  1.5989]],\n",
       "\n",
       "        [[-1.1832, -0.1410],\n",
       "         [ 0.5424,  1.5989],\n",
       "         [-1.1832, -0.1410]],\n",
       "\n",
       "        [[ 0.5424,  1.5989],\n",
       "         [-1.1832, -0.1410],\n",
       "         [-0.4192, -0.4529]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions are: n_samples x context_window x embedding_size\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50750e2",
   "metadata": {},
   "source": [
    "These were all examples, so what we bring from this section into the next part of the neural network is the **lookup table** as well as the **embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0768bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup table: vocab_size x embedding_dimension\n",
    "C = torch.randn([27, 2])\n",
    "\n",
    "# Embeddings\n",
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea596834",
   "metadata": {},
   "source": [
    "### Adding more layers to the network\n",
    "The different layers of the network fit together via their input- and output-dimensions. \n",
    "\n",
    "Examples of dimensions:  \n",
    "\n",
    "* samples: n_samples x context_window  \n",
    "* lookup table: vocab_size x embedding_size  \n",
    "* layer: (context_window * embedding_size) x n_neurons  \n",
    "\n",
    "After the samples have gone through the embedding layer we have a matrix pr sample of dimension context_window x embedding_size. Before we can multiply this output with the first neuron layer, we need to unstack it:\n",
    "\n",
    "<pre>\n",
    "[[1,2],\n",
    " [4,5],    ---> [1,2,3,4,5,6]\n",
    " [6,7]]\n",
    "</pre>\n",
    "\n",
    "Actually pytorch stores its data as a one-dimentional tensor all ready, and one can easily alter between the dimensionality using .view(). We will use that here to unstack the two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee87b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 1)\n",
    "W1 = torch.randn([6,100])\n",
    "b1 = torch.randn([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bba8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5]],\n",
      "\n",
      "        [[ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[12, 13],\n",
      "         [14, 15],\n",
      "         [16, 17]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "# Unstack example\n",
    "e1 = torch.arange(18); print(e1)\n",
    "e2 = e1.view(3,3,2); print(e2)\n",
    "e3 = e2.view(3,6); print(e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698b180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unstacking sample dimensions and calculating activations\n",
    "h = torch.tanh(emb.view(emb.size()[0], emb.size()[1]*emb.size()[2]) @ W1 + b1); h.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925792a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and biases (layer 2)\n",
    "W2 = torch.randn([100,27])\n",
    "b2 = torch.randn([27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716039e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating logits for each possible output\n",
    "logits = h @ W2 + b2; logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cbd8b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.7133,   4.4883,  -0.0745,   6.0326,  -2.7591,  13.3141,  12.2796,\n",
       "          1.3787,   9.9715,   2.5350,  -2.5225,   4.4464, -12.3316,  -5.0761,\n",
       "         -2.1436,  -7.8701,  -3.5819,  -4.4530,  -1.0093,  17.5434,  -1.2620,\n",
       "        -11.4313,   5.8831,  -5.7168,  -8.4261,  -5.3225,  -3.6467])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### CONTINUE HERE ####\n",
    "#### http://www.youtube.com/watch?v=TCH_1BHY58I&t=29m50s\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "## Calculating loss\n",
    "### Without regularization\n",
    "### loss = -probs[:,ys[:n_samples]].log().mean()\n",
    "### With regulatization (Rewarding low Ws)\n",
    "loss = -probs[:,ys[:n_samples]].log().mean() + 0.01*(W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c86bd61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

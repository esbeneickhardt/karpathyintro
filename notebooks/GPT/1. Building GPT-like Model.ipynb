{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71456801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 1, ' ': 2, '!': 3, '$': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '3': 10, ':': 11, ';': 12, '?': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, 'a': 40, 'b': 41, 'c': 42, 'd': 43, 'e': 44, 'f': 45, 'g': 46, 'h': 47, 'i': 48, 'j': 49, 'k': 50, 'l': 51, 'm': 52, 'n': 53, 'o': 54, 'p': 55, 'q': 56, 'r': 57, 's': 58, 't': 59, 'u': 60, 'v': 61, 'w': 62, 'x': 63, 'y': 64, 'z': 65}\n",
      "{1: '\\n', 2: ' ', 3: '!', 4: '$', 5: '&', 6: \"'\", 7: ',', 8: '-', 0: '.', 10: '3', 11: ':', 12: ';', 13: '?', 14: 'A', 15: 'B', 16: 'C', 17: 'D', 18: 'E', 19: 'F', 20: 'G', 21: 'H', 22: 'I', 23: 'J', 24: 'K', 25: 'L', 26: 'M', 27: 'N', 28: 'O', 29: 'P', 30: 'Q', 31: 'R', 32: 'S', 33: 'T', 34: 'U', 35: 'V', 36: 'W', 37: 'X', 38: 'Y', 39: 'Z', 40: 'a', 41: 'b', 42: 'c', 43: 'd', 44: 'e', 45: 'f', 46: 'g', 47: 'h', 48: 'i', 49: 'j', 50: 'k', 51: 'l', 52: 'm', 53: 'n', 54: 'o', 55: 'p', 56: 'q', 57: 'r', 58: 's', 59: 't', 60: 'u', 61: 'v', 62: 'w', 63: 'x', 64: 'y', 65: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary\n",
    "ctoi = {s:i+1 for i,s in enumerate(chars)}; print(ctoi)\n",
    "ctoi['.'] = 0\n",
    "itoc = {i:s for s,i in ctoi.items()}; print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 48, 48, 2, 59, 47, 44, 57, 44]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
      "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
      "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8a27ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE: http://www.youtube.com/watch?v=kCc8FmEb1nY&t=14m50s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

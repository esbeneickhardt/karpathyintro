{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71456801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer that uses characters rather than word-chunks to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}; print(ctoi)\n",
    "itoc = {i:s for s,i in ctoi.items()}; print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e92a1",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "When builing out model, we would like it to be able to generate text from as little a context as one character, but still up to a context of size **block_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2443ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is: 47\n",
      "when input is tensor([18, 47]) target is: 56\n",
      "when input is tensor([18, 47, 56]) target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Example of a sample\n",
    "block_size = 8\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d45194",
   "metadata": {},
   "source": [
    "We create a function for getting random batches from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dcabf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc840e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[51, 63,  1, 52, 53, 58, 43, 11],\n",
      "        [ 6,  0, 32, 53,  1, 50, 39, 63],\n",
      "        [ 1, 61, 43, 50, 41, 53, 51, 43],\n",
      "        [56, 43, 58, 58, 63,  1, 58, 39]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[63,  1, 52, 53, 58, 43, 11,  0],\n",
      "        [ 0, 32, 53,  1, 50, 39, 63,  1],\n",
      "        [61, 43, 50, 41, 53, 51, 43,  2],\n",
      "        [43, 58, 58, 63,  1, 58, 39, 50]])\n",
      "when input is [51] target is: 63\n",
      "when input is [51, 63] target is: 1\n",
      "when input is [51, 63, 1] target is: 52\n",
      "when input is [51, 63, 1, 52] target is: 53\n",
      "when input is [51, 63, 1, 52, 53] target is: 58\n",
      "when input is [51, 63, 1, 52, 53, 58] target is: 43\n",
      "when input is [51, 63, 1, 52, 53, 58, 43] target is: 11\n",
      "when input is [51, 63, 1, 52, 53, 58, 43, 11] target is: 0\n",
      "when input is [6] target is: 0\n",
      "when input is [6, 0] target is: 32\n",
      "when input is [6, 0, 32] target is: 53\n",
      "when input is [6, 0, 32, 53] target is: 1\n",
      "when input is [6, 0, 32, 53, 1] target is: 50\n",
      "when input is [6, 0, 32, 53, 1, 50] target is: 39\n",
      "when input is [6, 0, 32, 53, 1, 50, 39] target is: 63\n",
      "when input is [6, 0, 32, 53, 1, 50, 39, 63] target is: 1\n",
      "when input is [1] target is: 61\n",
      "when input is [1, 61] target is: 43\n",
      "when input is [1, 61, 43] target is: 50\n",
      "when input is [1, 61, 43, 50] target is: 41\n",
      "when input is [1, 61, 43, 50, 41] target is: 53\n",
      "when input is [1, 61, 43, 50, 41, 53] target is: 51\n",
      "when input is [1, 61, 43, 50, 41, 53, 51] target is: 43\n",
      "when input is [1, 61, 43, 50, 41, 53, 51, 43] target is: 2\n",
      "when input is [56] target is: 43\n",
      "when input is [56, 43] target is: 58\n",
      "when input is [56, 43, 58] target is: 58\n",
      "when input is [56, 43, 58, 58] target is: 63\n",
      "when input is [56, 43, 58, 58, 63] target is: 1\n",
      "when input is [56, 43, 58, 58, 63, 1] target is: 58\n",
      "when input is [56, 43, 58, 58, 63, 1, 58] target is: 39\n",
      "when input is [56, 43, 58, 58, 63, 1, 58, 39] target is: 50\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "batch_size = 4 \n",
    "block_size = 8 \n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# Neural Network: Part I\n",
    "Now we will start feeding the data into a neural network. We will just start by using the bigram model similar to the one we build previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15cdcc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[51, 63,  1, 52, 53, 58, 43, 11],\n",
      "        [ 6,  0, 32, 53,  1, 50, 39, 63],\n",
      "        [ 1, 61, 43, 50, 41, 53, 51, 43],\n",
      "        [56, 43, 58, 58, 63,  1, 58, 39]])\n",
      "tensor([[63,  1, 52, 53, 58, 43, 11,  0],\n",
      "        [ 0, 32, 53,  1, 50, 39, 63,  1],\n",
      "        [61, 43, 50, 41, 53, 51, 43,  2],\n",
      "        [43, 58, 58, 63,  1, 58, 39, 50]])\n"
     ]
    }
   ],
   "source": [
    "# A batch\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3187ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" Creating Embedding Table \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (BATCH, TIME, CHANNEL)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "575c7794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.174387269895637"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss \n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91f9104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7054, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Creating model and calculating loss\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa0f8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "A,YPtg.C:j$.E!.?$g d!L,YdPHUeaiUD? cimfEc?'?X\n",
      "fOYd,-Aythl.'JicgrttoXLvlQbCdetBOPPd&\n",
      "Pas?NHdzXTnnMM:m\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=100)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbdf3f",
   "metadata": {},
   "source": [
    "# Training a Model: Part I\n",
    "Here we are going to use the Adam optimizer instead or stochastic gratient descent, which we used earlier. The optimizer is basically how the gradients are updated. Before we simple updadted it in the following way: \n",
    "\n",
    "p.data += -lr * 0.01 * p.grad. \n",
    "\n",
    "Now instead the optimizer keeps track of the gradient-history, such that it can create momentum in a certain direction and converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36ec22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afbca72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.515717029571533\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ccd40ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "ADWent, athupy re un;\n",
      "F he palisisZ.3Q&.\n",
      "Se I gichey het s nt myon.\n",
      "\n",
      "HENoon, feder n shil tIshe hel'stind l:\n",
      "LLYe\n",
      "Fis br b'theindy be worveat ar Jer's toshies sarlinderthordes dotcousthingur\n",
      "\n",
      "\n",
      "TMeAu ifomefout aws irears hal vies than.\n",
      "TI wir g hef winomougivane, I marus,\n",
      "Anghe ale?\n",
      "\n",
      "I coundn! yor be\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8a73c",
   "metadata": {},
   "source": [
    "# Exporting Code\n",
    "Here we export all the code a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 4.\\ Neural\\ Network\\ Module.ipynb \"###NN###\" ../../modules/Micrograd/nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8a27ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUE: http://www.youtube.com/watch?v=kCc8FmEb1nY&t=38m11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

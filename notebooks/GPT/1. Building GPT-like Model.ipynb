{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb08b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "############\n",
    "### Data ###\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71456801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878d5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer that uses characters rather than word-chunks to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Priting vocabulary\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6261509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "# Printing example\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b3d0a",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "When builing out model, we would like it to be able to generate text from as little a context as one character, but still up to a context of size **block_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a092f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is: 47\n",
      "when input is tensor([18, 47]) target is: 56\n",
      "when input is tensor([18, 47, 56]) target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Example of a sample\n",
    "block_size = 8\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d079c",
   "metadata": {},
   "source": [
    "We create a function for getting random batches from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeb3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b6d70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[61, 53, 56, 43,  1, 51, 43,  1],\n",
      "        [13, 52, 42,  1, 21,  1, 41, 53],\n",
      "        [52,  1, 51, 39, 42, 43,  1, 39],\n",
      "        [16, 33, 15, 20, 17, 31, 31,  1]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 56, 43,  1, 51, 43,  1, 47],\n",
      "        [52, 42,  1, 21,  1, 41, 53, 59],\n",
      "        [ 1, 51, 39, 42, 43,  1, 39,  1],\n",
      "        [33, 15, 20, 17, 31, 31,  1, 27]])\n",
      "when input is [61] target is: 53\n",
      "when input is [61, 53] target is: 56\n",
      "when input is [61, 53, 56] target is: 43\n",
      "when input is [61, 53, 56, 43] target is: 1\n",
      "when input is [61, 53, 56, 43, 1] target is: 51\n",
      "when input is [61, 53, 56, 43, 1, 51] target is: 43\n",
      "when input is [61, 53, 56, 43, 1, 51, 43] target is: 1\n",
      "when input is [61, 53, 56, 43, 1, 51, 43, 1] target is: 47\n",
      "when input is [13] target is: 52\n",
      "when input is [13, 52] target is: 42\n",
      "when input is [13, 52, 42] target is: 1\n",
      "when input is [13, 52, 42, 1] target is: 21\n",
      "when input is [13, 52, 42, 1, 21] target is: 1\n",
      "when input is [13, 52, 42, 1, 21, 1] target is: 41\n",
      "when input is [13, 52, 42, 1, 21, 1, 41] target is: 53\n",
      "when input is [13, 52, 42, 1, 21, 1, 41, 53] target is: 59\n",
      "when input is [52] target is: 1\n",
      "when input is [52, 1] target is: 51\n",
      "when input is [52, 1, 51] target is: 39\n",
      "when input is [52, 1, 51, 39] target is: 42\n",
      "when input is [52, 1, 51, 39, 42] target is: 43\n",
      "when input is [52, 1, 51, 39, 42, 43] target is: 1\n",
      "when input is [52, 1, 51, 39, 42, 43, 1] target is: 39\n",
      "when input is [52, 1, 51, 39, 42, 43, 1, 39] target is: 1\n",
      "when input is [16] target is: 33\n",
      "when input is [16, 33] target is: 15\n",
      "when input is [16, 33, 15] target is: 20\n",
      "when input is [16, 33, 15, 20] target is: 17\n",
      "when input is [16, 33, 15, 20, 17] target is: 31\n",
      "when input is [16, 33, 15, 20, 17, 31] target is: 31\n",
      "when input is [16, 33, 15, 20, 17, 31, 31] target is: 1\n",
      "when input is [16, 33, 15, 20, 17, 31, 31, 1] target is: 27\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "batch_size = 4 \n",
    "block_size = 8 \n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# Neural Network: Part I\n",
    "Now we will start feeding the data into a neural network. We will just start by using the bigram model similar to the one we build previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427dc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[61, 53, 56, 43,  1, 51, 43,  1],\n",
      "        [13, 52, 42,  1, 21,  1, 41, 53],\n",
      "        [52,  1, 51, 39, 42, 43,  1, 39],\n",
      "        [16, 33, 15, 20, 17, 31, 31,  1]])\n",
      "tensor([[53, 56, 43,  1, 51, 43,  1, 47],\n",
      "        [52, 42,  1, 21,  1, 41, 53, 59],\n",
      "        [ 1, 51, 39, 42, 43,  1, 39,  1],\n",
      "        [33, 15, 20, 17, 31, 31,  1, 27]])\n"
     ]
    }
   ],
   "source": [
    "# A batch\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396fb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#############\n",
    "### Model ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ff31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" Creating Embedding Table \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (BATCH, TIME, CHANNEL)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc00aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f9690d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.174387269895637"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss \n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b347f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.3470, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Running forward pass of model\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403d4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "CEoR'Rz$IZBigvkr!Iu:dpKx&bgKj-ELgMPjQh&3kXux,-Z\n",
      "fGqG,XVbSI qFfMZBQh:z$Umtrd'ixilimPFTgvnreABbSlVgssn\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=100)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c3448",
   "metadata": {},
   "source": [
    "# Training Model: Part I\n",
    "Here we are going to use the Adam optimizer instead or stochastic gratient descent, which we used earlier. The optimizer is basically how the gradients are updated. Before we simple updadted it in the following way: \n",
    "\n",
    "p.data += -lr * 0.01 * p.grad. \n",
    "\n",
    "Now instead the optimizer keeps track of the gradient-history, such that it can create momentum in a certain direction and converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52d32189",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "################\n",
    "### Training ###\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c825fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56082155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4644, valid loss 4.4744\n",
      "step 300: train loss 2.7682, valid loss 2.7854\n",
      "step 600: train loss 2.5341, valid loss 2.5575\n",
      "step 900: train loss 2.5078, valid loss 2.5186\n",
      "step 1200: train loss 2.4829, valid loss 2.5011\n",
      "step 1500: train loss 2.4695, valid loss 2.5080\n",
      "step 1800: train loss 2.4646, valid loss 2.4958\n",
      "step 2100: train loss 2.4603, valid loss 2.4904\n",
      "step 2400: train loss 2.4585, valid loss 2.4918\n",
      "step 2700: train loss 2.4674, valid loss 2.4946\n",
      "step 3000: train loss 2.4611, valid loss 2.4858\n",
      "step 3300: train loss 2.4513, valid loss 2.4860\n",
      "step 3600: train loss 2.4680, valid loss 2.4998\n",
      "step 3900: train loss 2.4630, valid loss 2.4949\n",
      "step 4200: train loss 2.4551, valid loss 2.4890\n",
      "step 4500: train loss 2.4497, valid loss 2.4813\n",
      "step 4800: train loss 2.4535, valid loss 2.4888\n",
      "step 5100: train loss 2.4608, valid loss 2.4763\n",
      "step 5400: train loss 2.4650, valid loss 2.4894\n",
      "step 5700: train loss 2.4458, valid loss 2.4872\n",
      "step 6000: train loss 2.4543, valid loss 2.4782\n",
      "step 6300: train loss 2.4613, valid loss 2.4939\n",
      "step 6600: train loss 2.4674, valid loss 2.4987\n",
      "step 6900: train loss 2.4580, valid loss 2.4801\n",
      "step 7200: train loss 2.4572, valid loss 2.4769\n",
      "step 7500: train loss 2.4605, valid loss 2.4896\n",
      "step 7800: train loss 2.4619, valid loss 2.4860\n",
      "step 8100: train loss 2.4576, valid loss 2.4956\n",
      "step 8400: train loss 2.4571, valid loss 2.4833\n",
      "step 8700: train loss 2.4678, valid loss 2.4804\n",
      "step 9000: train loss 2.4628, valid loss 2.5069\n",
      "step 9300: train loss 2.4627, valid loss 2.4829\n",
      "step 9600: train loss 2.4566, valid loss 2.4866\n",
      "step 9900: train loss 2.4588, valid loss 2.4799\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "##################\n",
    "### Generating ###\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8276df4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Dico-h llkindo-beeshigachets\n",
      "\n",
      "Topinanistyowinengon fiinomy romprtus, as chotlan ad, te, omehainngees, afo?\n",
      "'ss ide r boofr h'sebe henes f the thanonur bl cor y ds t uif ne nodouromy, ghawounthet.\n",
      "CI'liepabl ll PHichars'TCERFinrnke thed t ff buacatoousles IOF amy, he hal.\n",
      "\n",
      "Ber oro-\n",
      "Mathaurceton f bu \n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc8d9f",
   "metadata": {},
   "source": [
    "# Self-Attention: Part I\n",
    "We will now build out the network by adding self-attention. We start out by making a minimal example illustrating what self-attention is.\n",
    "\n",
    "Example:  \n",
    "As we are going to predict the future the attention will work in the following way.\n",
    "\n",
    "* Tokens: abcdef\n",
    " * a: Cannot attend to any other characters than itself\n",
    " * b: Can attend only to a and b\n",
    " * c: Can attend to a, b and c\n",
    " * .......\n",
    " \n",
    "How to attend to different numbers of tokens can be done in various way, of which the most simple is just to average them. Here we will throw away a lot of information, but we start out this way for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6552eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11fca103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Calculating average of all previous tokens\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): \n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056f5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample:\n",
      " tensor([[ 1.7416,  0.1733],\n",
      "        [ 0.9289,  0.3166],\n",
      "        [ 1.2728, -1.1221],\n",
      "        [-0.0952, -0.0067],\n",
      "        [-0.7445, -0.2665],\n",
      "        [-1.5116, -0.8351],\n",
      "        [-0.3951, -1.3280],\n",
      "        [-0.3442,  0.0985]])\n",
      "output sample:\n",
      " tensor([[ 1.7416,  0.1733],\n",
      "        [ 1.3352,  0.2450],\n",
      "        [ 1.3144, -0.2107],\n",
      "        [ 0.9620, -0.1597],\n",
      "        [ 0.6207, -0.1811],\n",
      "        [ 0.2653, -0.2901],\n",
      "        [ 0.1710, -0.4383],\n",
      "        [ 0.1066, -0.3712]])\n",
      "token 1 average: tensor([1.7416, 0.1733])\n",
      "token 2 average: tensor([1.3352, 0.2450])\n",
      "token 3 average: tensor([ 1.3144, -0.2107])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a sample\n",
    "print(\"input sample:\\n\", x[0])\n",
    "print(\"output sample:\\n\", xbow[0])\n",
    "print(\"token 1 average:\", x[0][0])\n",
    "print(\"token 2 average:\", (x[0][0] + x[0][1])/2)\n",
    "print(\"token 3 average:\", (x[0][0] + x[0][1] + x[0][2])/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa212c21",
   "metadata": {},
   "source": [
    "For loops are slow, so now we will do it a lot faster using [matrix multiplication](http://matrixmultiplication.xyz/). Here the approach is shown via an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b:\n",
      "tensor([[4., 2.],\n",
      "        [5., 6.],\n",
      "        [9., 3.]])\n",
      "c:\n",
      "tensor([[4.0000, 2.0000],\n",
      "        [4.5000, 4.0000],\n",
      "        [6.0000, 3.6667]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True);print(\"a:\");print(a)\n",
    "b = torch.randint(0,10,(3,2)).float();print(\"b:\");print(b)\n",
    "c = a @ b;print(\"c:\");print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7613c",
   "metadata": {},
   "source": [
    "Replacing the for-loop with matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368777b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Calculating average of all previous tokens\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = torch.tril(torch.ones(T,T))/torch.sum(wei,1,keepdim=True)\n",
    "xbow2 = wei @ x;xbow2\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0e227",
   "metadata": {},
   "source": [
    "Because we will be implementing a more advanced attention system, we create a third version for calculating the same, just using soft-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Calculating average of all previous tokens\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x;xbow3\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e754",
   "metadata": {},
   "source": [
    "# Neural Network: Part II\n",
    "Here we make some adjustments to the BigramLanguageModel as well as add a positional embedding and implement the self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f813cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61886dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd3e417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15c84e",
   "metadata": {},
   "source": [
    "To the language model we add a linear layer and positional embeddings. We also complete the self attention implementation, which is better explained [here](https://jalammar.github.io/illustrated-transformer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "422ce60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Self-Attention Header\n",
    "# Sample\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "649f3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Head of Self-Attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6040eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Attention\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v;out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e920ded",
   "metadata": {},
   "source": [
    "Now we write the attention code for one head into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58b04008",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e31f5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bed9e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0567142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3749258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2076, valid loss 4.2047\n",
      "step 300: train loss 2.7935, valid loss 2.8063\n",
      "step 600: train loss 2.6050, valid loss 2.6074\n",
      "step 900: train loss 2.5266, valid loss 2.5466\n",
      "step 1200: train loss 2.4864, valid loss 2.4904\n",
      "step 1500: train loss 2.4634, valid loss 2.4754\n",
      "step 1800: train loss 2.4391, valid loss 2.4704\n",
      "step 2100: train loss 2.4374, valid loss 2.4522\n",
      "step 2400: train loss 2.4217, valid loss 2.4315\n",
      "step 2700: train loss 2.4259, valid loss 2.4363\n",
      "step 3000: train loss 2.4051, valid loss 2.4394\n",
      "step 3300: train loss 2.4064, valid loss 2.4329\n",
      "step 3600: train loss 2.4022, valid loss 2.4239\n",
      "step 3900: train loss 2.4054, valid loss 2.4269\n",
      "step 4200: train loss 2.4033, valid loss 2.4264\n",
      "step 4500: train loss 2.3867, valid loss 2.4115\n",
      "step 4800: train loss 2.3763, valid loss 2.4077\n",
      "step 5100: train loss 2.3963, valid loss 2.4252\n",
      "step 5400: train loss 2.3866, valid loss 2.4158\n",
      "step 5700: train loss 2.3689, valid loss 2.4022\n",
      "step 6000: train loss 2.3769, valid loss 2.4015\n",
      "step 6300: train loss 2.3738, valid loss 2.3942\n",
      "step 6600: train loss 2.3697, valid loss 2.3938\n",
      "step 6900: train loss 2.3678, valid loss 2.3915\n",
      "step 7200: train loss 2.3668, valid loss 2.3930\n",
      "step 7500: train loss 2.3622, valid loss 2.3884\n",
      "step 7800: train loss 2.3716, valid loss 2.3812\n",
      "step 8100: train loss 2.3702, valid loss 2.3768\n",
      "step 8400: train loss 2.3683, valid loss 2.3885\n",
      "step 8700: train loss 2.3652, valid loss 2.3907\n",
      "step 9000: train loss 2.3643, valid loss 2.3838\n",
      "step 9300: train loss 2.3660, valid loss 2.3879\n",
      "step 9600: train loss 2.3554, valid loss 2.3917\n",
      "step 9900: train loss 2.3663, valid loss 2.3769\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b15b2fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Pat biined woulinch wille. FRIOKIINLongo, cut Whe the lgoouprar siccesit ant crtiroff tha shereds se, ofnl ire omy youd, bofr dy theme ld woustat arinowou heirodrkse Ed.\n",
      "\n",
      "An:\n",
      "D:\n",
      "Youncke. BUngeand buth winof tert ouncto wast oci\n",
      "'Fexe e\n",
      "Be dow\n",
      "HAnng hen tornesmatur be.\n",
      "\n",
      "TICLARINof I bofrand no cerodo\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f247198",
   "metadata": {},
   "source": [
    "# Self-Attention: Part II\n",
    "Now we will add multiple attention heads to our model. Multi-headed attention is basically to run the data through multiple attention headers and then concatenating the results. Here we implement this, but first all the boiler-plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4d27f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f30ec62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f59c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb3a8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10441a1",
   "metadata": {},
   "source": [
    "Here we create the multi-head class that simple makes several copies of the head layer and concatenates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41936b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2920e042",
   "metadata": {},
   "source": [
    "Finally we make minor adjustments to the BigramLanguageModel such that it uses the multi-headed attention during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb807f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb83f0",
   "metadata": {},
   "source": [
    "The training loop and evaluation is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a0f95cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5383cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92403222",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1430, valid loss 4.1497\n",
      "step 300: train loss 2.8712, valid loss 2.8880\n",
      "step 600: train loss 2.6664, valid loss 2.6811\n",
      "step 900: train loss 2.5963, valid loss 2.5987\n",
      "step 1200: train loss 2.5268, valid loss 2.5376\n",
      "step 1500: train loss 2.4864, valid loss 2.5051\n",
      "step 1800: train loss 2.4638, valid loss 2.4671\n",
      "step 2100: train loss 2.4254, valid loss 2.4513\n",
      "step 2400: train loss 2.4149, valid loss 2.4182\n",
      "step 2700: train loss 2.3849, valid loss 2.3975\n",
      "step 3000: train loss 2.3898, valid loss 2.3844\n",
      "step 3300: train loss 2.3597, valid loss 2.3770\n",
      "step 3600: train loss 2.3382, valid loss 2.3529\n",
      "step 3900: train loss 2.3350, valid loss 2.3554\n",
      "step 4200: train loss 2.3222, valid loss 2.3376\n",
      "step 4500: train loss 2.3135, valid loss 2.3300\n",
      "step 4800: train loss 2.3070, valid loss 2.3256\n",
      "step 5100: train loss 2.2965, valid loss 2.3303\n",
      "step 5400: train loss 2.2856, valid loss 2.3189\n",
      "step 5700: train loss 2.2666, valid loss 2.2961\n",
      "step 6000: train loss 2.2727, valid loss 2.2948\n",
      "step 6300: train loss 2.2545, valid loss 2.2919\n",
      "step 6600: train loss 2.2614, valid loss 2.2955\n",
      "step 6900: train loss 2.2519, valid loss 2.2809\n",
      "step 7200: train loss 2.2377, valid loss 2.2873\n",
      "step 7500: train loss 2.2491, valid loss 2.2795\n",
      "step 7800: train loss 2.2506, valid loss 2.2764\n",
      "step 8100: train loss 2.2228, valid loss 2.2708\n",
      "step 8400: train loss 2.2417, valid loss 2.2717\n",
      "step 8700: train loss 2.2185, valid loss 2.2566\n",
      "step 9000: train loss 2.2198, valid loss 2.2585\n",
      "step 9300: train loss 2.2291, valid loss 2.2712\n",
      "step 9600: train loss 2.2226, valid loss 2.2431\n",
      "step 9900: train loss 2.2165, valid loss 2.2452\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfad68f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "MERTHARD ISINCE:\n",
      "'s thou be\n",
      "Rod tret youe\n",
      "Saplanadsel. Wir, u Shy osnk, ning way rim londor but, oflay wy rostr lor, I MAnd thean thist, no, losth, wew--\n",
      "ISIO:\n",
      "Le nefu sthed hat hat murpood, blrll plow?\n",
      "\n",
      "Youth dl-tat, sant dind me'llith lavellluld, so her?\n",
      "Pund wat yem.\n",
      "\n",
      "'s gre vrow brave bewles my \n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7396427",
   "metadata": {},
   "source": [
    "# Adding Non-Linearity\n",
    "So far we have just added attention, but we do not use any activation functions. Here we are going to add non-liniarities, but first all the boiler-plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "425ad54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3004530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eb3373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf44da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c038579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8fd1f",
   "metadata": {},
   "source": [
    "Here we add the FeedForwad class with has a linear layer and uses the ReLU non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "732c45cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "841ce67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffw(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "beb1120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d16f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60ae6a55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1964, valid loss 4.1970\n",
      "step 300: train loss 2.8224, valid loss 2.8138\n",
      "step 600: train loss 2.6042, valid loss 2.5844\n",
      "step 900: train loss 2.5112, valid loss 2.4988\n",
      "step 1200: train loss 2.4571, valid loss 2.4619\n",
      "step 1500: train loss 2.4227, valid loss 2.4124\n",
      "step 1800: train loss 2.3956, valid loss 2.3911\n",
      "step 2100: train loss 2.3784, valid loss 2.3801\n",
      "step 2400: train loss 2.3549, valid loss 2.3611\n",
      "step 2700: train loss 2.3323, valid loss 2.3542\n",
      "step 3000: train loss 2.3159, valid loss 2.3200\n",
      "step 3300: train loss 2.2955, valid loss 2.3132\n",
      "step 3600: train loss 2.3026, valid loss 2.3021\n",
      "step 3900: train loss 2.2760, valid loss 2.2985\n",
      "step 4200: train loss 2.2680, valid loss 2.3031\n",
      "step 4500: train loss 2.2610, valid loss 2.2731\n",
      "step 4800: train loss 2.2526, valid loss 2.2746\n",
      "step 5100: train loss 2.2503, valid loss 2.2737\n",
      "step 5400: train loss 2.2388, valid loss 2.2845\n",
      "step 5700: train loss 2.2255, valid loss 2.2618\n",
      "step 6000: train loss 2.2214, valid loss 2.2461\n",
      "step 6300: train loss 2.2136, valid loss 2.2567\n",
      "step 6600: train loss 2.2045, valid loss 2.2447\n",
      "step 6900: train loss 2.2032, valid loss 2.2425\n",
      "step 7200: train loss 2.2052, valid loss 2.2252\n",
      "step 7500: train loss 2.1949, valid loss 2.2319\n",
      "step 7800: train loss 2.1803, valid loss 2.2287\n",
      "step 8100: train loss 2.1832, valid loss 2.2181\n",
      "step 8400: train loss 2.1752, valid loss 2.2285\n",
      "step 8700: train loss 2.1774, valid loss 2.2240\n",
      "step 9000: train loss 2.1819, valid loss 2.2207\n",
      "step 9300: train loss 2.1741, valid loss 2.2085\n",
      "step 9600: train loss 2.1586, valid loss 2.2124\n",
      "step 9900: train loss 2.1529, valid loss 2.1980\n"
     ]
    }
   ],
   "source": [
    "###GPTNONLINEARITY###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "051e2b94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Thil,\n",
      "\n",
      "This lid sted tiespeald.\n",
      "\n",
      "ADEYET:\n",
      "Hu'ldid ovend spise thealtreve?\n",
      "Hods tir;\n",
      "I yourssirly hit fla now lit foreve, bt of ose heraive.\n",
      "\n",
      "LEOLUSH:\n",
      "Yooursefaingr?\n",
      "Bome yourstele,\n",
      "And; sin, I'lffordy, sorotht enfo of ut hallivit, I nim frifuld. Olfis whain?\n",
      "\n",
      "BAM:\n",
      "And lewie!\n",
      "\n",
      "MENWecalie pand weam,\n",
      "Am\n"
     ]
    }
   ],
   "source": [
    "###GPTNONLINEARITY###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36389505",
   "metadata": {},
   "source": [
    "# Creating Transformer Block\n",
    "We simply write the transformer code into a block such that we easily can create multiple transformer layers for the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4074f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e58b2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8880f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f00c5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a6be0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2fe8fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40d624d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" Transformer block \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Adding Attention and ffw to X \"\"\"\n",
    "        x = self.sa(x)\n",
    "        x = self.ffw(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92815048",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "764d6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ee90db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f5b31ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1668, valid loss 4.1652\n",
      "step 300: train loss 3.2417, valid loss 3.2755\n",
      "step 600: train loss 3.1686, valid loss 3.1690\n",
      "step 900: train loss 3.1087, valid loss 3.1022\n",
      "step 1200: train loss 3.0841, valid loss 3.0657\n",
      "step 1500: train loss 3.0528, valid loss 3.0341\n",
      "step 1800: train loss 3.0426, valid loss 3.0312\n",
      "step 2100: train loss 3.0283, valid loss 3.0083\n",
      "step 2400: train loss 2.9133, valid loss 2.9059\n",
      "step 2700: train loss 2.7460, valid loss 2.7412\n",
      "step 3000: train loss 2.6890, valid loss 2.6744\n",
      "step 3300: train loss 2.6295, valid loss 2.6031\n",
      "step 3600: train loss 2.5676, valid loss 2.5682\n",
      "step 3900: train loss 2.5386, valid loss 2.5289\n",
      "step 4200: train loss 2.5205, valid loss 2.5148\n",
      "step 4500: train loss 2.5055, valid loss 2.5126\n",
      "step 4800: train loss 2.4767, valid loss 2.4687\n",
      "step 5100: train loss 2.4727, valid loss 2.4660\n",
      "step 5400: train loss 2.4552, valid loss 2.4613\n",
      "step 5700: train loss 2.4504, valid loss 2.4400\n",
      "step 6000: train loss 2.4355, valid loss 2.4341\n",
      "step 6300: train loss 2.4167, valid loss 2.4301\n",
      "step 6600: train loss 2.4145, valid loss 2.4165\n",
      "step 6900: train loss 2.4137, valid loss 2.4226\n",
      "step 7200: train loss 2.4001, valid loss 2.3988\n",
      "step 7500: train loss 2.4009, valid loss 2.3853\n",
      "step 7800: train loss 2.3870, valid loss 2.3866\n",
      "step 8100: train loss 2.3697, valid loss 2.3811\n",
      "step 8400: train loss 2.3670, valid loss 2.3620\n",
      "step 8700: train loss 2.3538, valid loss 2.3573\n",
      "step 9000: train loss 2.3759, valid loss 2.3585\n",
      "step 9300: train loss 2.3429, valid loss 2.3540\n",
      "step 9600: train loss 2.3401, valid loss 2.3534\n",
      "step 9900: train loss 2.3413, valid loss 2.3377\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTILAYER###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c81affe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "To the a ler fosghof com,\n",
      "I, pol thibenem levey ire non:\n",
      "Hons stof;\n",
      "Tepe than my theand,\n",
      "Yot, ames met, want, fufer the I'sothe ansod britrule toickes gut wor'dunime sof nobecom. A \n",
      "os. Mo brut he:\n",
      "Wam nisf bor mo, farses'ed, ipalr exriven frardoon mer the\n",
      "Cangs avand limbay insimods brois, thont;\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTILAYER###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8bc1f",
   "metadata": {},
   "source": [
    "# Adding Residual Connections, Normalization and Dropout\n",
    "Here we add the different innovations from the transformer paper:\n",
    "\n",
    "* Residual Connection\n",
    "  * It is basically just adding two matrices together. This is done in the **Block** class two times.\n",
    "* Layer Normalization\n",
    "  * It is very similar to batch normalization ensuring zero mean and 1 variance, it just normalized rows instead of columns. We add it in **Block**.\n",
    "* Dropout\n",
    "  * Randomly prevents some of the nodes from communicating. This has a regularizing effect, as the network is forced to make more robust connections. Implemented in **Head**, **MultiHeadAttention** and **FeedForward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28688901",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a039f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 64\n",
    "block_size = 164\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 3e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 \n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "33f9edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98c3cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53f16e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed * num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fef3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "131245af",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" Transformer block \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # Have trainable paramters gamma and beta \n",
    "        self.ln2 = nn.LayerNorm(n_embed) # Have trainable paramters gamma and beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Adding Attention, ffw and pre-layernorm to X \"\"\"\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffw(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fd6093ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05bda329",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35703185",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dddf37f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10496x32 and 128x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Once in a while evaluate loss on train and valid sets\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valid loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[95], line 17\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     16\u001b[0m     X,Y \u001b[38;5;241m=\u001b[39m get_batch(split, batch_size, block_size)\n\u001b[0;32m---> 17\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     19\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[94], line 25\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     23\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[93], line 16\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Adding Attention, ffw and pre-layernorm to X \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffw(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[91], line 15\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Calculating and Concatenating Results \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/karpathyintro/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10496x32 and 128x32)"
     ]
    }
   ],
   "source": [
    "###GPTCONNECT###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258873a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c053d5",
   "metadata": {},
   "source": [
    "# Exporting Code\n",
    "Here we export code labeled with ###BIGRAM###, ###GPTHEAD### and so on to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###BIGRAM###\" ../../modules/GPT/bigram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTHEAD###\" ../../modules/GPT/GPThead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ded958",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTMULTIHEAD###\" ../../modules/GPT/GPTmultihead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5491a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTNONLINEARITY###\" ../../modules/GPT/GPTnonlinearity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ee9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTMULTILAYER###\" ../../modules/GPT/GPTmultilayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "703586f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cells with label ###GPTCONNECT### extracted from 1. Building GPT-like Model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTCONNECT###\" ../../modules/GPT/GPT.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb08b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "############\n",
    "### Data ###\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71456801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878d5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer that uses characters rather than word-chunks to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Priting vocabulary\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6261509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "# Printing example\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b3d0a",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "When builing out model, we would like it to be able to generate text from as little a context as one character, but still up to a context of size **block_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a092f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is: 47\n",
      "when input is tensor([18, 47]) target is: 56\n",
      "when input is tensor([18, 47, 56]) target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Example of a sample\n",
    "block_size = 8\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d079c",
   "metadata": {},
   "source": [
    "We create a function for getting random batches from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeb3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b6d70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[41, 43, 10,  0, 13, 52, 42,  1],\n",
      "        [43, 39, 60, 43,  6,  1, 51, 63],\n",
      "        [42,  1, 40, 63,  1, 39, 52,  1],\n",
      "        [44, 50, 53, 61, 43, 56,  1, 53]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 10,  0, 13, 52, 42,  1, 58],\n",
      "        [39, 60, 43,  6,  1, 51, 63,  1],\n",
      "        [ 1, 40, 63,  1, 39, 52,  1, 43],\n",
      "        [50, 53, 61, 43, 56,  1, 53, 44]])\n",
      "when input is [41] target is: 43\n",
      "when input is [41, 43] target is: 10\n",
      "when input is [41, 43, 10] target is: 0\n",
      "when input is [41, 43, 10, 0] target is: 13\n",
      "when input is [41, 43, 10, 0, 13] target is: 52\n",
      "when input is [41, 43, 10, 0, 13, 52] target is: 42\n",
      "when input is [41, 43, 10, 0, 13, 52, 42] target is: 1\n",
      "when input is [41, 43, 10, 0, 13, 52, 42, 1] target is: 58\n",
      "when input is [43] target is: 39\n",
      "when input is [43, 39] target is: 60\n",
      "when input is [43, 39, 60] target is: 43\n",
      "when input is [43, 39, 60, 43] target is: 6\n",
      "when input is [43, 39, 60, 43, 6] target is: 1\n",
      "when input is [43, 39, 60, 43, 6, 1] target is: 51\n",
      "when input is [43, 39, 60, 43, 6, 1, 51] target is: 63\n",
      "when input is [43, 39, 60, 43, 6, 1, 51, 63] target is: 1\n",
      "when input is [42] target is: 1\n",
      "when input is [42, 1] target is: 40\n",
      "when input is [42, 1, 40] target is: 63\n",
      "when input is [42, 1, 40, 63] target is: 1\n",
      "when input is [42, 1, 40, 63, 1] target is: 39\n",
      "when input is [42, 1, 40, 63, 1, 39] target is: 52\n",
      "when input is [42, 1, 40, 63, 1, 39, 52] target is: 1\n",
      "when input is [42, 1, 40, 63, 1, 39, 52, 1] target is: 43\n",
      "when input is [44] target is: 50\n",
      "when input is [44, 50] target is: 53\n",
      "when input is [44, 50, 53] target is: 61\n",
      "when input is [44, 50, 53, 61] target is: 43\n",
      "when input is [44, 50, 53, 61, 43] target is: 56\n",
      "when input is [44, 50, 53, 61, 43, 56] target is: 1\n",
      "when input is [44, 50, 53, 61, 43, 56, 1] target is: 53\n",
      "when input is [44, 50, 53, 61, 43, 56, 1, 53] target is: 44\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "batch_size = 4 \n",
    "block_size = 8 \n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# Neural Network: Part I\n",
    "Now we will start feeding the data into a neural network. We will just start by using the bigram model similar to the one we build previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427dc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[41, 43, 10,  0, 13, 52, 42,  1],\n",
      "        [43, 39, 60, 43,  6,  1, 51, 63],\n",
      "        [42,  1, 40, 63,  1, 39, 52,  1],\n",
      "        [44, 50, 53, 61, 43, 56,  1, 53]])\n",
      "tensor([[43, 10,  0, 13, 52, 42,  1, 58],\n",
      "        [39, 60, 43,  6,  1, 51, 63,  1],\n",
      "        [ 1, 40, 63,  1, 39, 52,  1, 43],\n",
      "        [50, 53, 61, 43, 56,  1, 53, 44]])\n"
     ]
    }
   ],
   "source": [
    "# A batch\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396fb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#############\n",
    "### Model ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ff31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" Creating Embedding Table \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (BATCH, TIME, CHANNEL)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc00aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f9690d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.174387269895637"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss \n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b347f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6661, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Running forward pass of model\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403d4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "JmB&rz'$:JB$;Wq'AxoRds-LGtz'lEs$QUeL;.TP&u?YSGtC.S\n",
      "dBAd?Cmc'I\n",
      "zhjjI$;PZ,syNWmKZVIAqfdsydTWq&EBOhlbCu\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=100)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c3448",
   "metadata": {},
   "source": [
    "# Training Model: Part I\n",
    "Here we are going to use the Adam optimizer instead or stochastic gratient descent, which we used earlier. The optimizer is basically how the gradients are updated. Before we simple updadted it in the following way: \n",
    "\n",
    "p.data += -lr * 0.01 * p.grad. \n",
    "\n",
    "Now instead the optimizer keeps track of the gradient-history, such that it can create momentum in a certain direction and converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52d32189",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "################\n",
    "### Training ###\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c825fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56082155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6304, valid loss 4.6278\n",
      "step 300: train loss 2.7788, valid loss 2.7900\n",
      "step 600: train loss 2.5481, valid loss 2.5574\n",
      "step 900: train loss 2.5033, valid loss 2.5183\n",
      "step 1200: train loss 2.4770, valid loss 2.5049\n",
      "step 1500: train loss 2.4720, valid loss 2.5031\n",
      "step 1800: train loss 2.4689, valid loss 2.4987\n",
      "step 2100: train loss 2.4720, valid loss 2.5002\n",
      "step 2400: train loss 2.4614, valid loss 2.4877\n",
      "step 2700: train loss 2.4658, valid loss 2.4895\n",
      "step 3000: train loss 2.4698, valid loss 2.4915\n",
      "step 3300: train loss 2.4578, valid loss 2.4960\n",
      "step 3600: train loss 2.4536, valid loss 2.4883\n",
      "step 3900: train loss 2.4663, valid loss 2.4928\n",
      "step 4200: train loss 2.4656, valid loss 2.4936\n",
      "step 4500: train loss 2.4567, valid loss 2.4689\n",
      "step 4800: train loss 2.4535, valid loss 2.4915\n",
      "step 5100: train loss 2.4496, valid loss 2.4875\n",
      "step 5400: train loss 2.4694, valid loss 2.4951\n",
      "step 5700: train loss 2.4588, valid loss 2.4840\n",
      "step 6000: train loss 2.4579, valid loss 2.4923\n",
      "step 6300: train loss 2.4555, valid loss 2.4856\n",
      "step 6600: train loss 2.4532, valid loss 2.4888\n",
      "step 6900: train loss 2.4537, valid loss 2.4795\n",
      "step 7200: train loss 2.4484, valid loss 2.4860\n",
      "step 7500: train loss 2.4613, valid loss 2.4987\n",
      "step 7800: train loss 2.4566, valid loss 2.4912\n",
      "step 8100: train loss 2.4549, valid loss 2.4881\n",
      "step 8400: train loss 2.4603, valid loss 2.4950\n",
      "step 8700: train loss 2.4583, valid loss 2.4818\n",
      "step 9000: train loss 2.4595, valid loss 2.4959\n",
      "step 9300: train loss 2.4489, valid loss 2.4895\n",
      "step 9600: train loss 2.4587, valid loss 2.4915\n",
      "step 9900: train loss 2.4589, valid loss 2.4890\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "##################\n",
    "### Generating ###\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8276df4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Anessourd deay'ld CHay\n",
      "T:\n",
      "MPRYolthy. lal,\n",
      "n! t hisith he w m\n",
      "GLE cowinecr w:\n",
      "MENGUMot\n",
      "\n",
      "thase ngor.\n",
      "He aped my totr.\n",
      "Loflmim y meeeostre apushuno\n",
      "Thare, wabt ha-l\n",
      "LO, hidishe-gais;\n",
      "Y: ney osd, k,\n",
      "WI w; blothithent w Boursannthathind,\n",
      "\n",
      "HE s the ham me INERene ina l ansheatt R:\n",
      "\n",
      "Of r ol the RETht ad? s\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc8d9f",
   "metadata": {},
   "source": [
    "# Self-Attention: Part I\n",
    "We will now build out the network by adding self-attention. We start out by making a minimal example illustrating what self-attention is.\n",
    "\n",
    "Example:  \n",
    "As we are going to predict the future the attention will work in the following way.\n",
    "\n",
    "* Tokens: abcdef\n",
    " * a: Cannot attend to any other characters than itself\n",
    " * b: Can attend only to a and b\n",
    " * c: Can attend to a, b and c\n",
    " * .......\n",
    " \n",
    "How to attend to different numbers of tokens can be done in various way, of which the most simple is just to average them. Here we will throw away a lot of information, but we start out this way for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6552eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11fca103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Calculating average of all previous tokens\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): \n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056f5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample:\n",
      " tensor([[-0.5447, -0.9758],\n",
      "        [-0.7237, -1.6342],\n",
      "        [ 0.6724, -0.4791],\n",
      "        [ 1.0421,  0.5574],\n",
      "        [ 0.6697, -1.0027],\n",
      "        [ 0.2482, -0.3644],\n",
      "        [-1.5257, -2.6367],\n",
      "        [ 0.8385,  0.5197]])\n",
      "output sample:\n",
      " tensor([[-0.5447, -0.9758],\n",
      "        [-0.6342, -1.3050],\n",
      "        [-0.1986, -1.0297],\n",
      "        [ 0.1115, -0.6329],\n",
      "        [ 0.2232, -0.7069],\n",
      "        [ 0.2273, -0.6498],\n",
      "        [-0.0231, -0.9336],\n",
      "        [ 0.0846, -0.7520]])\n",
      "token 1 average: tensor([-0.5447, -0.9758])\n",
      "token 2 average: tensor([-0.6342, -1.3050])\n",
      "token 3 average: tensor([-0.1986, -1.0297])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a sample\n",
    "print(\"input sample:\\n\", x[0])\n",
    "print(\"output sample:\\n\", xbow[0])\n",
    "print(\"token 1 average:\", x[0][0])\n",
    "print(\"token 2 average:\", (x[0][0] + x[0][1])/2)\n",
    "print(\"token 3 average:\", (x[0][0] + x[0][1] + x[0][2])/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa212c21",
   "metadata": {},
   "source": [
    "For loops are slow, so now we will do it a lot faster using [matrix multiplication](http://matrixmultiplication.xyz/). Here the approach is shown via an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b:\n",
      "tensor([[2., 5.],\n",
      "        [1., 3.],\n",
      "        [6., 3.]])\n",
      "c:\n",
      "tensor([[2.0000, 5.0000],\n",
      "        [1.5000, 4.0000],\n",
      "        [3.0000, 3.6667]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True);print(\"a:\");print(a)\n",
    "b = torch.randint(0,10,(3,2)).float();print(\"b:\");print(b)\n",
    "c = a @ b;print(\"c:\");print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7613c",
   "metadata": {},
   "source": [
    "Replacing the for-loop with matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368777b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Calculating average of all previous tokens\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = torch.tril(torch.ones(T,T))/torch.sum(wei,1,keepdim=True)\n",
    "xbow2 = wei @ x;xbow2\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0e227",
   "metadata": {},
   "source": [
    "Because we will be implementing a more advanced attention system, we create a third version for calculating the same, just using soft-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Calculating average of all previous tokens\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x;xbow3\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e754",
   "metadata": {},
   "source": [
    "# Neural Network: Part II\n",
    "Here we make some adjustments to the BigraLanguageModel as well as add a positional embedding and implement the self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f813cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61886dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bd3e417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15c84e",
   "metadata": {},
   "source": [
    "To the language model we add a linear layer and positional embeddings. We also complete the self attention implementation, which is better explained [here](https://jalammar.github.io/illustrated-transformer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "422ce60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Self-Attention Header\n",
    "# Sample\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "649f3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Head of Self-Attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6040eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Attention\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v;out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e920ded",
   "metadata": {},
   "source": [
    "Now we write the attention code for one head into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "58b04008",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e31f5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bed9e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0567142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM2###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f3749258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1926, valid loss 4.1916\n",
      "step 300: train loss 2.8904, valid loss 2.9071\n",
      "step 600: train loss 2.6325, valid loss 2.6226\n",
      "step 900: train loss 2.5349, valid loss 2.5284\n",
      "step 1200: train loss 2.4998, valid loss 2.4901\n",
      "step 1500: train loss 2.4821, valid loss 2.4734\n",
      "step 1800: train loss 2.4653, valid loss 2.4597\n",
      "step 2100: train loss 2.4393, valid loss 2.4385\n",
      "step 2400: train loss 2.4415, valid loss 2.4279\n",
      "step 2700: train loss 2.4234, valid loss 2.4217\n",
      "step 3000: train loss 2.4187, valid loss 2.4230\n",
      "step 3300: train loss 2.4070, valid loss 2.4141\n",
      "step 3600: train loss 2.4063, valid loss 2.4182\n",
      "step 3900: train loss 2.3978, valid loss 2.4123\n",
      "step 4200: train loss 2.3916, valid loss 2.4102\n",
      "step 4500: train loss 2.3958, valid loss 2.3947\n",
      "step 4800: train loss 2.3813, valid loss 2.3838\n",
      "step 5100: train loss 2.3973, valid loss 2.3875\n",
      "step 5400: train loss 2.3663, valid loss 2.3992\n",
      "step 5700: train loss 2.3731, valid loss 2.3952\n",
      "step 6000: train loss 2.3757, valid loss 2.3882\n",
      "step 6300: train loss 2.3736, valid loss 2.3903\n",
      "step 6600: train loss 2.3715, valid loss 2.3933\n",
      "step 6900: train loss 2.3696, valid loss 2.3922\n",
      "step 7200: train loss 2.3723, valid loss 2.3829\n",
      "step 7500: train loss 2.3792, valid loss 2.3792\n",
      "step 7800: train loss 2.3590, valid loss 2.3842\n",
      "step 8100: train loss 2.3637, valid loss 2.3867\n",
      "step 8400: train loss 2.3521, valid loss 2.3933\n",
      "step 8700: train loss 2.3620, valid loss 2.3767\n",
      "step 9000: train loss 2.3698, valid loss 2.3810\n",
      "step 9300: train loss 2.3614, valid loss 2.3747\n",
      "step 9600: train loss 2.3551, valid loss 2.3820\n",
      "step 9900: train loss 2.3559, valid loss 2.3643\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM2###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8b15b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "I'\n",
      "HEY: ndemyeagrey san thet thut nt they has Pben gohorw And ow do sos\n",
      "Arorer, gohe tco'my!\n",
      "t\n",
      "White am: Inte ws cenesp linou rse re Burie-t klantee nd, stilany ulle gthe, thofr thefu Whe vir con'd.\n",
      "Hot me fo the adsh Eurve, itam, I utrint uas yor gacks buby, thangigo te you\n",
      "Mane coult,\n",
      "Tont VZLoull\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM2###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41372149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=kCc8FmEb1nY&t=82m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c053d5",
   "metadata": {},
   "source": [
    "# Exporting Code\n",
    "Here we export code labeled with ###BIGRAM###, ###BIGRAM2### and so on to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###BIGRAM###\" ../../modules/GPT/bigram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2b68f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cells with label ###BIGRAM2### extracted from 1. Building GPT-like Model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###BIGRAM2###\" ../../modules/GPT/bigram2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

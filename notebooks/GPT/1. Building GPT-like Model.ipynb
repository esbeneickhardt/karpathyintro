{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb08b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "############\n",
    "### Data ###\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71456801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878d5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer that uses characters rather than word-chunks to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Priting vocabulary\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6261509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "# Printing example\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b3d0a",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "When builing out model, we would like it to be able to generate text from as little a context as one character, but still up to a context of size **block_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a092f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is: 47\n",
      "when input is tensor([18, 47]) target is: 56\n",
      "when input is tensor([18, 47, 56]) target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Example of a sample\n",
    "block_size = 8\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d079c",
   "metadata": {},
   "source": [
    "We create a function for getting random batches from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeb3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b6d70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 56, 51, 57,  1, 58, 46, 39],\n",
      "        [ 1, 50, 43, 58,  1, 51, 63,  1],\n",
      "        [58, 46,  2,  0,  0, 16, 33, 23],\n",
      "        [40, 53, 58, 46,  1, 53, 44,  1]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[56, 51, 57,  1, 58, 46, 39, 58],\n",
      "        [50, 43, 58,  1, 51, 63,  1, 53],\n",
      "        [46,  2,  0,  0, 16, 33, 23, 17],\n",
      "        [53, 58, 46,  1, 53, 44,  1, 63]])\n",
      "when input is [53] target is: 56\n",
      "when input is [53, 56] target is: 51\n",
      "when input is [53, 56, 51] target is: 57\n",
      "when input is [53, 56, 51, 57] target is: 1\n",
      "when input is [53, 56, 51, 57, 1] target is: 58\n",
      "when input is [53, 56, 51, 57, 1, 58] target is: 46\n",
      "when input is [53, 56, 51, 57, 1, 58, 46] target is: 39\n",
      "when input is [53, 56, 51, 57, 1, 58, 46, 39] target is: 58\n",
      "when input is [1] target is: 50\n",
      "when input is [1, 50] target is: 43\n",
      "when input is [1, 50, 43] target is: 58\n",
      "when input is [1, 50, 43, 58] target is: 1\n",
      "when input is [1, 50, 43, 58, 1] target is: 51\n",
      "when input is [1, 50, 43, 58, 1, 51] target is: 63\n",
      "when input is [1, 50, 43, 58, 1, 51, 63] target is: 1\n",
      "when input is [1, 50, 43, 58, 1, 51, 63, 1] target is: 53\n",
      "when input is [58] target is: 46\n",
      "when input is [58, 46] target is: 2\n",
      "when input is [58, 46, 2] target is: 0\n",
      "when input is [58, 46, 2, 0] target is: 0\n",
      "when input is [58, 46, 2, 0, 0] target is: 16\n",
      "when input is [58, 46, 2, 0, 0, 16] target is: 33\n",
      "when input is [58, 46, 2, 0, 0, 16, 33] target is: 23\n",
      "when input is [58, 46, 2, 0, 0, 16, 33, 23] target is: 17\n",
      "when input is [40] target is: 53\n",
      "when input is [40, 53] target is: 58\n",
      "when input is [40, 53, 58] target is: 46\n",
      "when input is [40, 53, 58, 46] target is: 1\n",
      "when input is [40, 53, 58, 46, 1] target is: 53\n",
      "when input is [40, 53, 58, 46, 1, 53] target is: 44\n",
      "when input is [40, 53, 58, 46, 1, 53, 44] target is: 1\n",
      "when input is [40, 53, 58, 46, 1, 53, 44, 1] target is: 63\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "batch_size = 4 \n",
    "block_size = 8 \n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# Neural Network: Part I\n",
    "Now we will start feeding the data into a neural network. We will just start by using the bigram model similar to the one we build previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427dc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 56, 51, 57,  1, 58, 46, 39],\n",
      "        [ 1, 50, 43, 58,  1, 51, 63,  1],\n",
      "        [58, 46,  2,  0,  0, 16, 33, 23],\n",
      "        [40, 53, 58, 46,  1, 53, 44,  1]])\n",
      "tensor([[56, 51, 57,  1, 58, 46, 39, 58],\n",
      "        [50, 43, 58,  1, 51, 63,  1, 53],\n",
      "        [46,  2,  0,  0, 16, 33, 23, 17],\n",
      "        [53, 58, 46,  1, 53, 44,  1, 63]])\n"
     ]
    }
   ],
   "source": [
    "# A batch\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396fb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#############\n",
    "### Model ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ff31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" Creating Embedding Table \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (BATCH, TIME, CHANNEL)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc00aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f9690d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.174387269895637"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss \n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b347f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8196, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Running forward pass of model\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403d4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "dKezqas!wAjlvCaWm?\n",
      "$iNAyg\n",
      "pzn&YBxWW?.RpJRLZ$Eavx&DbZtWcSZ hnF!aO:d!&smxERrp\n",
      "GnQiJAb cpGMYTE,3&Sa:,Et\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=100)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c3448",
   "metadata": {},
   "source": [
    "# Training Model: Part I\n",
    "Here we are going to use the Adam optimizer instead or stochastic gratient descent, which we used earlier. The optimizer is basically how the gradients are updated. Before we simple updadted it in the following way: \n",
    "\n",
    "p.data += -lr * 0.01 * p.grad. \n",
    "\n",
    "Now instead the optimizer keeps track of the gradient-history, such that it can create momentum in a certain direction and converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52d32189",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "################\n",
    "### Training ###\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c825fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56082155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5905, valid loss 4.5717\n",
      "step 300: train loss 2.7891, valid loss 2.8027\n",
      "step 600: train loss 2.5396, valid loss 2.5569\n",
      "step 900: train loss 2.4952, valid loss 2.5058\n",
      "step 1200: train loss 2.4872, valid loss 2.5027\n",
      "step 1500: train loss 2.4839, valid loss 2.4951\n",
      "step 1800: train loss 2.4714, valid loss 2.4960\n",
      "step 2100: train loss 2.4589, valid loss 2.4892\n",
      "step 2400: train loss 2.4727, valid loss 2.4947\n",
      "step 2700: train loss 2.4614, valid loss 2.5005\n",
      "step 3000: train loss 2.4592, valid loss 2.4945\n",
      "step 3300: train loss 2.4623, valid loss 2.4881\n",
      "step 3600: train loss 2.4519, valid loss 2.4919\n",
      "step 3900: train loss 2.4648, valid loss 2.4795\n",
      "step 4200: train loss 2.4615, valid loss 2.4736\n",
      "step 4500: train loss 2.4570, valid loss 2.4932\n",
      "step 4800: train loss 2.4598, valid loss 2.4930\n",
      "step 5100: train loss 2.4619, valid loss 2.4840\n",
      "step 5400: train loss 2.4586, valid loss 2.4868\n",
      "step 5700: train loss 2.4543, valid loss 2.4773\n",
      "step 6000: train loss 2.4607, valid loss 2.4862\n",
      "step 6300: train loss 2.4615, valid loss 2.4765\n",
      "step 6600: train loss 2.4530, valid loss 2.4821\n",
      "step 6900: train loss 2.4511, valid loss 2.4887\n",
      "step 7200: train loss 2.4528, valid loss 2.4798\n",
      "step 7500: train loss 2.4615, valid loss 2.4855\n",
      "step 7800: train loss 2.4474, valid loss 2.4756\n",
      "step 8100: train loss 2.4553, valid loss 2.4810\n",
      "step 8400: train loss 2.4598, valid loss 2.4966\n",
      "step 8700: train loss 2.4614, valid loss 2.4964\n",
      "step 9000: train loss 2.4503, valid loss 2.4875\n",
      "step 9300: train loss 2.4546, valid loss 2.4869\n",
      "step 9600: train loss 2.4650, valid loss 2.4925\n",
      "step 9900: train loss 2.4565, valid loss 2.4941\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "##################\n",
    "### Generating ###\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8276df4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "PS:\n",
      "Catethar roor STo haves.\n",
      "AD:\n",
      "IORULeas paze enilo t w\n",
      "Thenty,\n",
      "QUp, vomyoul?\n",
      "\n",
      "Rorse shate:\n",
      "\n",
      "TAs, winth,\n",
      "Whabumpofof cu al heduch whehasacepitr totro myo t bu she imaif que is yoknsothe own h; giler ueyofoneyor friest th aby hath he\n",
      "beselens thed t iend tes clat bof ty I dd wofernind O f gr toudo, \n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc8d9f",
   "metadata": {},
   "source": [
    "# Self-Attention: Part I\n",
    "We will now build out the network by adding self-attention. We start out by making a minimal example illustrating what self-attention is.\n",
    "\n",
    "Example:  \n",
    "As we are going to predict the future the attention will work in the following way.\n",
    "\n",
    "* Tokens: abcdef\n",
    " * a: Cannot attend to any other characters than itself\n",
    " * b: Can attend only to a and b\n",
    " * c: Can attend to a, b and c\n",
    " * .......\n",
    " \n",
    "How to attend to different numbers of tokens can be done in various way, of which the most simple is just to average them. Here we will throw away a lot of information, but we start out this way for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6552eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11fca103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Calculating average of all previous tokens\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): \n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056f5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample:\n",
      " tensor([[-0.9394,  2.5976],\n",
      "        [-0.2369, -0.4511],\n",
      "        [ 0.4705,  0.9367],\n",
      "        [ 0.6360,  0.1003],\n",
      "        [-0.0450, -1.6891],\n",
      "        [ 1.7757, -0.2522],\n",
      "        [ 0.1259,  0.6234],\n",
      "        [-1.1404,  0.4708]])\n",
      "output sample:\n",
      " tensor([[-0.9394,  2.5976],\n",
      "        [-0.5881,  1.0733],\n",
      "        [-0.2353,  1.0278],\n",
      "        [-0.0174,  0.7959],\n",
      "        [-0.0230,  0.2989],\n",
      "        [ 0.2768,  0.2071],\n",
      "        [ 0.2553,  0.2665],\n",
      "        [ 0.0808,  0.2921]])\n",
      "token 1 average: tensor([-0.9394,  2.5976])\n",
      "token 2 average: tensor([-0.5881,  1.0733])\n",
      "token 3 average: tensor([-0.2353,  1.0278])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a sample\n",
    "print(\"input sample:\\n\", x[0])\n",
    "print(\"output sample:\\n\", xbow[0])\n",
    "print(\"token 1 average:\", x[0][0])\n",
    "print(\"token 2 average:\", (x[0][0] + x[0][1])/2)\n",
    "print(\"token 3 average:\", (x[0][0] + x[0][1] + x[0][2])/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa212c21",
   "metadata": {},
   "source": [
    "For loops are slow, so now we will do it a lot faster using [matrix multiplication](http://matrixmultiplication.xyz/). Here the approach is shown via an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b:\n",
      "tensor([[1., 2.],\n",
      "        [9., 2.],\n",
      "        [5., 2.]])\n",
      "c:\n",
      "tensor([[1., 2.],\n",
      "        [5., 2.],\n",
      "        [5., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True);print(\"a:\");print(a)\n",
    "b = torch.randint(0,10,(3,2)).float();print(\"b:\");print(b)\n",
    "c = a @ b;print(\"c:\");print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7613c",
   "metadata": {},
   "source": [
    "Replacing the for-loop with matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368777b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Calculating average of all previous tokens\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = torch.tril(torch.ones(T,T))/torch.sum(wei,1,keepdim=True)\n",
    "xbow2 = wei @ x;xbow2\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0e227",
   "metadata": {},
   "source": [
    "Because we will be implementing a more advanced attention system, we create a third version for calculating the same, just using soft-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Calculating average of all previous tokens\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x;xbow3\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e754",
   "metadata": {},
   "source": [
    "# Neural Network: Part II\n",
    "Here we make some adjustments to the BigramLanguageModel as well as add a positional embedding and implement the self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f813cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61886dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd3e417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15c84e",
   "metadata": {},
   "source": [
    "To the language model we add a linear layer and positional embeddings. We also complete the self attention implementation, which is better explained [here](https://jalammar.github.io/illustrated-transformer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "422ce60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Self-Attention Header\n",
    "# Sample\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "649f3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Head of Self-Attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6040eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Attention\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v;out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e920ded",
   "metadata": {},
   "source": [
    "Now we write the attention code for one head into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58b04008",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e31f5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bed9e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0567142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3749258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1273, valid loss 4.1317\n",
      "step 300: train loss 2.9050, valid loss 2.9306\n",
      "step 600: train loss 2.6385, valid loss 2.6372\n",
      "step 900: train loss 2.5488, valid loss 2.5465\n",
      "step 1200: train loss 2.5070, valid loss 2.5043\n",
      "step 1500: train loss 2.4769, valid loss 2.4812\n",
      "step 1800: train loss 2.4551, valid loss 2.4669\n",
      "step 2100: train loss 2.4348, valid loss 2.4565\n",
      "step 2400: train loss 2.4283, valid loss 2.4313\n",
      "step 2700: train loss 2.4173, valid loss 2.4030\n",
      "step 3000: train loss 2.4181, valid loss 2.4233\n",
      "step 3300: train loss 2.4094, valid loss 2.4091\n",
      "step 3600: train loss 2.3986, valid loss 2.4132\n",
      "step 3900: train loss 2.3962, valid loss 2.4054\n",
      "step 4200: train loss 2.4019, valid loss 2.4105\n",
      "step 4500: train loss 2.4050, valid loss 2.3967\n",
      "step 4800: train loss 2.3939, valid loss 2.3939\n",
      "step 5100: train loss 2.3943, valid loss 2.3997\n",
      "step 5400: train loss 2.3942, valid loss 2.3814\n",
      "step 5700: train loss 2.3791, valid loss 2.3926\n",
      "step 6000: train loss 2.3810, valid loss 2.3801\n",
      "step 6300: train loss 2.3750, valid loss 2.3848\n",
      "step 6600: train loss 2.3780, valid loss 2.3819\n",
      "step 6900: train loss 2.3627, valid loss 2.3904\n",
      "step 7200: train loss 2.3744, valid loss 2.3819\n",
      "step 7500: train loss 2.3672, valid loss 2.3812\n",
      "step 7800: train loss 2.3799, valid loss 2.3823\n",
      "step 8100: train loss 2.3637, valid loss 2.3780\n",
      "step 8400: train loss 2.3543, valid loss 2.3811\n",
      "step 8700: train loss 2.3699, valid loss 2.3860\n",
      "step 9000: train loss 2.3726, valid loss 2.3748\n",
      "step 9300: train loss 2.3584, valid loss 2.3819\n",
      "step 9600: train loss 2.3628, valid loss 2.3750\n",
      "step 9900: train loss 2.3585, valid loss 2.3825\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b15b2fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "CHAn fimad hat wongor hes; in be she ha hre, inoongo what? ativearee I cod,\n",
      "\n",
      "SI brenggle wime?\n",
      "\n",
      "Thal:\n",
      "KI sagrooun hour, the eand, atrkenngeer, my mge peard wies,\n",
      "BRe:\n",
      "MI: heay,\n",
      "An, coun ars,\n",
      "\n",
      "Wit, owwe!\n",
      "\n",
      "INUMKIHARS:\n",
      "I:\n",
      "A:\n",
      "Nell.\n",
      "OMO:\n",
      "r'd, owto spivete tapfe hare pore iveat otre by loum,\n",
      "A wor;\n",
      "AILABu\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c08fa9",
   "metadata": {},
   "source": [
    "# Self-Attention: Part II\n",
    "Now we will add multiple attention heads to our model. Multi-headed attention is basically to run the data through multiple attention headers and then concatenating the results. Here we implement this, but first all the boiler-plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "978bc7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea83c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad401496",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d7beda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0967dd5",
   "metadata": {},
   "source": [
    "Here we create the multi-head class that simple makes several copies of the head layer and concatenates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25f2b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc23395",
   "metadata": {},
   "source": [
    "Finally we make minor adjustments to the BigramLanguageModel such that it uses the multi-headed attention during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4fbb8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c07944",
   "metadata": {},
   "source": [
    "The training loop and evaluation is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "49ab10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c5b1162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ae4a0fd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2183, valid loss 4.2240\n",
      "step 300: train loss 2.8098, valid loss 2.8290\n",
      "step 600: train loss 2.6040, valid loss 2.6045\n",
      "step 900: train loss 2.5177, valid loss 2.5117\n",
      "step 1200: train loss 2.4568, valid loss 2.4699\n",
      "step 1500: train loss 2.4259, valid loss 2.4373\n",
      "step 1800: train loss 2.4052, valid loss 2.3980\n",
      "step 2100: train loss 2.3732, valid loss 2.3774\n",
      "step 2400: train loss 2.3624, valid loss 2.3747\n",
      "step 2700: train loss 2.3413, valid loss 2.3543\n",
      "step 3000: train loss 2.3261, valid loss 2.3280\n",
      "step 3300: train loss 2.3031, valid loss 2.3290\n",
      "step 3600: train loss 2.3078, valid loss 2.3350\n",
      "step 3900: train loss 2.2907, valid loss 2.3128\n",
      "step 4200: train loss 2.2878, valid loss 2.3171\n",
      "step 4500: train loss 2.2854, valid loss 2.3060\n",
      "step 4800: train loss 2.2698, valid loss 2.2992\n",
      "step 5100: train loss 2.2627, valid loss 2.2857\n",
      "step 5400: train loss 2.2507, valid loss 2.2821\n",
      "step 5700: train loss 2.2471, valid loss 2.2895\n",
      "step 6000: train loss 2.2573, valid loss 2.2797\n",
      "step 6300: train loss 2.2508, valid loss 2.2527\n",
      "step 6600: train loss 2.2284, valid loss 2.2603\n",
      "step 6900: train loss 2.2300, valid loss 2.2581\n",
      "step 7200: train loss 2.2344, valid loss 2.2630\n",
      "step 7500: train loss 2.2312, valid loss 2.2515\n",
      "step 7800: train loss 2.2099, valid loss 2.2500\n",
      "step 8100: train loss 2.2113, valid loss 2.2524\n",
      "step 8400: train loss 2.2123, valid loss 2.2329\n",
      "step 8700: train loss 2.2176, valid loss 2.2313\n",
      "step 9000: train loss 2.1917, valid loss 2.2285\n",
      "step 9300: train loss 2.2021, valid loss 2.2311\n",
      "step 9600: train loss 2.1839, valid loss 2.2378\n",
      "step 9900: train loss 2.1977, valid loss 2.2301\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d99410a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "lif't mines.\n",
      "F Nrais is staigh,\n",
      "Head\n",
      "F:\n",
      "Wentce mousesty,\n",
      "No tue king ducer, thyaw the, Gem inetimy sals Bubperuck! aik war:\n",
      "Welly heompe of thourry fack toour ncrd?\n",
      "\n",
      "GHOR:\n",
      "Wilt fad beance, willied ben ow, repplard, with,\n",
      "TSeirk.\n",
      "\n",
      "ISANT: My, onind.\n",
      "I Geaven so of thond\n",
      "Agive; you grot lichy shang thi\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41372149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=kCc8FmEb1nY&t=82m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c053d5",
   "metadata": {},
   "source": [
    "# Exporting Code\n",
    "Here we export code labeled with ###BIGRAM###, ###GPTHEAD### and so on to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "293aca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cells with label ###BIGRAM### extracted from 1. Building GPT-like Model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###BIGRAM###\" ../../modules/GPT/bigram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2b68f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cells with label ###GPTHEAD### extracted from 1. Building GPT-like Model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTHEAD###\" ../../modules/GPT/GPThead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cd890bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Cells with label ###GPTMULTIHEAD### extracted from 1. Building GPT-like Model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTMULTIHEAD###\" ../../modules/GPT/GPTmultihead.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

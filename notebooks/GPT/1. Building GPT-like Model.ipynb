{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcccb2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Here we will be building a GPT-like model from scratch based on the two papers [Attention is All You Need](https://arxiv.org/abs/1706.03762), which proposed the **transformer** architecture, and [GPT-3](https://arxiv.org/abs/2005.14165). GPT is a language model that given an input simple predicts the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b9455",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea51ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb08b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d21b8",
   "metadata": {},
   "source": [
    "# Data\n",
    "We import the tiny-Shakespeare dataset and process it such that it can be used for creating a model that can create Shakespeare texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d956fd",
   "metadata": {},
   "source": [
    "### Reading and Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "############\n",
    "### Data ###\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3d112e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c9570dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "# First 300 characters\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71456801",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878d5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "Vocab Chars: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"Vocab Chars: {''.join(chars):}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d0613",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Encoder/Decoder\n",
    "We just use a character-level tokenizer here, but in practice people e.g. OpenAI uses something else e.g. the **BPE** tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f67004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI encoder/decoder\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b118032",
   "metadata": {},
   "source": [
    "We will be using a simple tokenizer that uses characters rather than word-chunks to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8c6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "058928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Priting vocabulary\n",
    "print(ctoi)\n",
    "print(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6112255",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110caba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Testing encoder/decoder\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d005b",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "Using our simple tokenizer we tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479d2033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6261509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "# Printing example\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418172cc",
   "metadata": {},
   "source": [
    "### Train/Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42605b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b3d0a",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "When builing out model, we would like it to be able to generate text from as little a context as one character, but still up to a context of size **block_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a092f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) target is: 47\n",
      "when input is tensor([18, 47]) target is: 56\n",
      "when input is tensor([18, 47, 56]) target is: 57\n",
      "when input is tensor([18, 47, 56, 57]) target is: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) target is: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) target is: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) target is: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Example of a sample\n",
    "block_size = 8\n",
    "x = data_train[:block_size]\n",
    "y = data_train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d079c",
   "metadata": {},
   "source": [
    "We create a function for getting random batches from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeb3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b6d70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 41, 55, 59, 39, 47, 52, 58],\n",
      "        [43, 57, 58,  8,  0,  0, 16, 33],\n",
      "        [59, 43,  5, 42,  1, 51, 63,  1],\n",
      "        [46, 43,  1, 39, 54, 54, 50, 39]])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[41, 55, 59, 39, 47, 52, 58, 43],\n",
      "        [57, 58,  8,  0,  0, 16, 33, 23],\n",
      "        [43,  5, 42,  1, 51, 63,  1, 44],\n",
      "        [43,  1, 39, 54, 54, 50, 39, 59]])\n",
      "when input is [39] target is: 41\n",
      "when input is [39, 41] target is: 55\n",
      "when input is [39, 41, 55] target is: 59\n",
      "when input is [39, 41, 55, 59] target is: 39\n",
      "when input is [39, 41, 55, 59, 39] target is: 47\n",
      "when input is [39, 41, 55, 59, 39, 47] target is: 52\n",
      "when input is [39, 41, 55, 59, 39, 47, 52] target is: 58\n",
      "when input is [39, 41, 55, 59, 39, 47, 52, 58] target is: 43\n",
      "when input is [43] target is: 57\n",
      "when input is [43, 57] target is: 58\n",
      "when input is [43, 57, 58] target is: 8\n",
      "when input is [43, 57, 58, 8] target is: 0\n",
      "when input is [43, 57, 58, 8, 0] target is: 0\n",
      "when input is [43, 57, 58, 8, 0, 0] target is: 16\n",
      "when input is [43, 57, 58, 8, 0, 0, 16] target is: 33\n",
      "when input is [43, 57, 58, 8, 0, 0, 16, 33] target is: 23\n",
      "when input is [59] target is: 43\n",
      "when input is [59, 43] target is: 5\n",
      "when input is [59, 43, 5] target is: 42\n",
      "when input is [59, 43, 5, 42] target is: 1\n",
      "when input is [59, 43, 5, 42, 1] target is: 51\n",
      "when input is [59, 43, 5, 42, 1, 51] target is: 63\n",
      "when input is [59, 43, 5, 42, 1, 51, 63] target is: 1\n",
      "when input is [59, 43, 5, 42, 1, 51, 63, 1] target is: 44\n",
      "when input is [46] target is: 43\n",
      "when input is [46, 43] target is: 1\n",
      "when input is [46, 43, 1] target is: 39\n",
      "when input is [46, 43, 1, 39] target is: 54\n",
      "when input is [46, 43, 1, 39, 54] target is: 54\n",
      "when input is [46, 43, 1, 39, 54, 54] target is: 50\n",
      "when input is [46, 43, 1, 39, 54, 54, 50] target is: 39\n",
      "when input is [46, 43, 1, 39, 54, 54, 50, 39] target is: 59\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "batch_size = 4 \n",
    "block_size = 8 \n",
    "\n",
    "xb, yb = get_batch('train', batch_size, block_size)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc78c6",
   "metadata": {},
   "source": [
    "# Neural Network: Part I\n",
    "Now we will start feeding the data into a neural network. We will just start by using the bigram model similar to the one we build previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "427dc0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[39, 41, 55, 59, 39, 47, 52, 58],\n",
      "        [43, 57, 58,  8,  0,  0, 16, 33],\n",
      "        [59, 43,  5, 42,  1, 51, 63,  1],\n",
      "        [46, 43,  1, 39, 54, 54, 50, 39]])\n",
      "tensor([[41, 55, 59, 39, 47, 52, 58, 43],\n",
      "        [57, 58,  8,  0,  0, 16, 33, 23],\n",
      "        [43,  5, 42,  1, 51, 63,  1, 44],\n",
      "        [43,  1, 39, 54, 54, 50, 39, 59]])\n"
     ]
    }
   ],
   "source": [
    "# A batch\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396fb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "#############\n",
    "### Model ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71ff31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\" Creating Embedding Table \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        logits = self.token_embedding_table(idx) # (BATCH, TIME, CHANNEL)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc00aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f9690d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.174387269895637"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected loss \n",
    "-math.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bd850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b347f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6371, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Running forward pass of model\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "403d4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "maYacbM?ZFyRcx-\n",
      "DDK\n",
      "qLNUkHW;jYYtV.aPKJd!o$helg$eTE-A: XdQv3;$SHUuropObpoPxhehPtlFFurx&bnju zO'M'ZCVp\n"
     ]
    }
   ],
   "source": [
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=100)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c3448",
   "metadata": {},
   "source": [
    "# Training Model: Part I\n",
    "Here we are going to use the Adam optimizer instead or stochastic gratient descent, which we used earlier. The optimizer is basically how the gradients are updated. Before we simple updadted it in the following way: \n",
    "\n",
    "p.data += -lr * 0.01 * p.grad. \n",
    "\n",
    "Now instead the optimizer keeps track of the gradient-history, such that it can create momentum in a certain direction and converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52d32189",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "################\n",
    "### Training ###\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c825fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac306a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56082155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6793, valid loss 4.6836\n",
      "step 300: train loss 2.8199, valid loss 2.8321\n",
      "step 600: train loss 2.5559, valid loss 2.5617\n",
      "step 900: train loss 2.4975, valid loss 2.5172\n",
      "step 1200: train loss 2.4851, valid loss 2.5030\n",
      "step 1500: train loss 2.4629, valid loss 2.4922\n",
      "step 1800: train loss 2.4685, valid loss 2.4906\n",
      "step 2100: train loss 2.4721, valid loss 2.4764\n",
      "step 2400: train loss 2.4656, valid loss 2.4942\n",
      "step 2700: train loss 2.4647, valid loss 2.4891\n",
      "step 3000: train loss 2.4662, valid loss 2.4872\n",
      "step 3300: train loss 2.4668, valid loss 2.4809\n",
      "step 3600: train loss 2.4565, valid loss 2.4901\n",
      "step 3900: train loss 2.4676, valid loss 2.4935\n",
      "step 4200: train loss 2.4640, valid loss 2.4891\n",
      "step 4500: train loss 2.4588, valid loss 2.4906\n",
      "step 4800: train loss 2.4501, valid loss 2.4853\n",
      "step 5100: train loss 2.4582, valid loss 2.4909\n",
      "step 5400: train loss 2.4679, valid loss 2.4805\n",
      "step 5700: train loss 2.4519, valid loss 2.4919\n",
      "step 6000: train loss 2.4581, valid loss 2.4773\n",
      "step 6300: train loss 2.4486, valid loss 2.4841\n",
      "step 6600: train loss 2.4602, valid loss 2.4941\n",
      "step 6900: train loss 2.4586, valid loss 2.4908\n",
      "step 7200: train loss 2.4540, valid loss 2.4901\n",
      "step 7500: train loss 2.4544, valid loss 2.4902\n",
      "step 7800: train loss 2.4596, valid loss 2.4814\n",
      "step 8100: train loss 2.4608, valid loss 2.4826\n",
      "step 8400: train loss 2.4509, valid loss 2.4977\n",
      "step 8700: train loss 2.4581, valid loss 2.4886\n",
      "step 9000: train loss 2.4588, valid loss 2.4983\n",
      "step 9300: train loss 2.4553, valid loss 2.4947\n",
      "step 9600: train loss 2.4524, valid loss 2.4861\n",
      "step 9900: train loss 2.4501, valid loss 2.4908\n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###BIGRAM###\n",
    "##################\n",
    "### Generating ###\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8276df4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "\n",
      "ARIIOMy nd finorearedeether g eay sw thaustetire ES: beasg winsous\n",
      "Vofreawhe on are Andite, ere,\n",
      "Whawh thmellliseaqu iorare:\n",
      "Y it thomeorou, med MENTEYo ay I e sth:\n",
      "ICICo,\n",
      "An thitit yeucemok, a anithomior ouceamantu shole, r my KEMNourered s s lainthacudytogendwen:\n",
      "Whil\n",
      "TE fllon, men se I thid mee \n"
     ]
    }
   ],
   "source": [
    "###BIGRAM###\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc8d9f",
   "metadata": {},
   "source": [
    "# Self-Attention: Part I\n",
    "We will now build out the network by adding self-attention. We start out by making a minimal example illustrating what self-attention is.\n",
    "\n",
    "Example:  \n",
    "As we are going to predict the future the attention will work in the following way.\n",
    "\n",
    "* Tokens: abcdef\n",
    " * a: Cannot attend to any other characters than itself\n",
    " * b: Can attend only to a and b\n",
    " * c: Can attend to a, b and c\n",
    " * .......\n",
    " \n",
    "How to attend to different numbers of tokens can be done in various way, of which the most simple is just to average them. Here we will throw away a lot of information, but we start out this way for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6552eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11fca103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: Calculating average of all previous tokens\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B): \n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "056f5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sample:\n",
      " tensor([[-1.9903,  0.7617],\n",
      "        [ 0.8435, -0.6335],\n",
      "        [-0.6518, -0.7260],\n",
      "        [ 3.1971,  1.0723],\n",
      "        [ 0.1188,  1.3957],\n",
      "        [-0.7644,  0.0765],\n",
      "        [ 1.4463, -0.0962],\n",
      "        [-0.3653,  1.2909]])\n",
      "output sample:\n",
      " tensor([[-1.9903,  0.7617],\n",
      "        [-0.5734,  0.0641],\n",
      "        [-0.5995, -0.1993],\n",
      "        [ 0.3496,  0.1186],\n",
      "        [ 0.3035,  0.3740],\n",
      "        [ 0.1255,  0.3245],\n",
      "        [ 0.3142,  0.2644],\n",
      "        [ 0.2292,  0.3927]])\n",
      "token 1 average: tensor([-1.9903,  0.7617])\n",
      "token 2 average: tensor([-0.5734,  0.0641])\n",
      "token 3 average: tensor([-0.5995, -0.1993])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting a sample\n",
    "print(\"input sample:\\n\", x[0])\n",
    "print(\"output sample:\\n\", xbow[0])\n",
    "print(\"token 1 average:\", x[0][0])\n",
    "print(\"token 2 average:\", (x[0][0] + x[0][1])/2)\n",
    "print(\"token 3 average:\", (x[0][0] + x[0][1] + x[0][2])/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa212c21",
   "metadata": {},
   "source": [
    "For loops are slow, so now we will do it a lot faster using [matrix multiplication](http://matrixmultiplication.xyz/). Here the approach is shown via an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89ed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b:\n",
      "tensor([[1., 0.],\n",
      "        [3., 6.],\n",
      "        [4., 6.]])\n",
      "c:\n",
      "tensor([[1.0000, 0.0000],\n",
      "        [2.0000, 3.0000],\n",
      "        [2.6667, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a,1,keepdim=True);print(\"a:\");print(a)\n",
    "b = torch.randint(0,10,(3,2)).float();print(\"b:\");print(b)\n",
    "c = a @ b;print(\"c:\");print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7613c",
   "metadata": {},
   "source": [
    "Replacing the for-loop with matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368777b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 2: Calculating average of all previous tokens\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = torch.tril(torch.ones(T,T))/torch.sum(wei,1,keepdim=True)\n",
    "xbow2 = wei @ x;xbow2\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0e227",
   "metadata": {},
   "source": [
    "Because we will be implementing a more advanced attention system, we create a third version for calculating the same, just using soft-max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c87107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Calculating average of all previous tokens\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x;xbow3\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459e754",
   "metadata": {},
   "source": [
    "# Neural Network: Part II\n",
    "Here we make some adjustments to the BigramLanguageModel as well as add a positional embedding and implement the self-attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f813cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61886dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd3e417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15c84e",
   "metadata": {},
   "source": [
    "To the language model we add a linear layer and positional embeddings. We also complete the self attention implementation, which is better explained [here](https://jalammar.github.io/illustrated-transformer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "422ce60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4: Self-Attention Header\n",
    "# Sample\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "649f3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Head of Self-Attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6040eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-Attention\n",
    "tril = torch.tril(torch.ones(T,T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v;out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e920ded",
   "metadata": {},
   "source": [
    "Now we write the attention code for one head into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58b04008",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e31f5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bed9e49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0567142",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3749258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2140, valid loss 4.2121\n",
      "step 300: train loss 2.8634, valid loss 2.8531\n",
      "step 600: train loss 2.6230, valid loss 2.6169\n",
      "step 900: train loss 2.5330, valid loss 2.5386\n",
      "step 1200: train loss 2.4936, valid loss 2.5163\n",
      "step 1500: train loss 2.4777, valid loss 2.4781\n",
      "step 1800: train loss 2.4550, valid loss 2.4716\n",
      "step 2100: train loss 2.4433, valid loss 2.4556\n",
      "step 2400: train loss 2.4320, valid loss 2.4438\n",
      "step 2700: train loss 2.4164, valid loss 2.4408\n",
      "step 3000: train loss 2.4226, valid loss 2.4274\n",
      "step 3300: train loss 2.4227, valid loss 2.4269\n",
      "step 3600: train loss 2.4070, valid loss 2.4214\n",
      "step 3900: train loss 2.3984, valid loss 2.4262\n",
      "step 4200: train loss 2.3968, valid loss 2.4151\n",
      "step 4500: train loss 2.3927, valid loss 2.4037\n",
      "step 4800: train loss 2.3840, valid loss 2.4040\n",
      "step 5100: train loss 2.3821, valid loss 2.4080\n",
      "step 5400: train loss 2.3850, valid loss 2.3969\n",
      "step 5700: train loss 2.3812, valid loss 2.4014\n",
      "step 6000: train loss 2.3860, valid loss 2.3870\n",
      "step 6300: train loss 2.3745, valid loss 2.4000\n",
      "step 6600: train loss 2.3665, valid loss 2.3932\n",
      "step 6900: train loss 2.3723, valid loss 2.3898\n",
      "step 7200: train loss 2.3751, valid loss 2.3791\n",
      "step 7500: train loss 2.3679, valid loss 2.3901\n",
      "step 7800: train loss 2.3680, valid loss 2.3879\n",
      "step 8100: train loss 2.3587, valid loss 2.3777\n",
      "step 8400: train loss 2.3573, valid loss 2.3898\n",
      "step 8700: train loss 2.3601, valid loss 2.3812\n",
      "step 9000: train loss 2.3564, valid loss 2.3906\n",
      "step 9300: train loss 2.3616, valid loss 2.3900\n",
      "step 9600: train loss 2.3604, valid loss 2.3859\n",
      "step 9900: train loss 2.3585, valid loss 2.3921\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b15b2fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Whome rant, orthileakisd wonethe en,\n",
      "As wheranen mout at ult was fin y-che borveve, at lcot meset y urd, ars of warve itunamp!\n",
      "An a t I Hencad bous be's haleat by\n",
      "I mier's we se youlll by ath caset, wer hanul wand ayou t\n",
      "IMit thoure hith.\n",
      "HAncesysel, tred, my s, half akacatonndel pos, moughm\n",
      "Thidean\n"
     ]
    }
   ],
   "source": [
    "###GPTHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79215605",
   "metadata": {},
   "source": [
    "# Self-Attention: Part II\n",
    "Now we will add multiple attention heads to our model. Multi-headed attention is basically to run the data through multiple attention headers and then concatenating the results. Here we implement this, but first all the boiler-plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55039416",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d9789d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de61c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "084f0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46fc3c4",
   "metadata": {},
   "source": [
    "Here we create the multi-head class that simple makes several copies of the head layer and concatenates the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e495b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f85849",
   "metadata": {},
   "source": [
    "Finally we make minor adjustments to the BigramLanguageModel such that it uses the multi-headed attention during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b96e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450f06d",
   "metadata": {},
   "source": [
    "The training loop and evaluation is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf67a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0df8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTIHEAD###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c36cf284",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3186, valid loss 4.3102\n",
      "step 300: train loss 2.8007, valid loss 2.8097\n",
      "step 600: train loss 2.6013, valid loss 2.5953\n",
      "step 900: train loss 2.5277, valid loss 2.5124\n",
      "step 1200: train loss 2.4651, valid loss 2.4687\n",
      "step 1500: train loss 2.4275, valid loss 2.4369\n",
      "step 1800: train loss 2.3942, valid loss 2.4200\n",
      "step 2100: train loss 2.3778, valid loss 2.3951\n",
      "step 2400: train loss 2.3644, valid loss 2.3788\n",
      "step 2700: train loss 2.3460, valid loss 2.3672\n",
      "step 3000: train loss 2.3289, valid loss 2.3503\n",
      "step 3300: train loss 2.3180, valid loss 2.3472\n",
      "step 3600: train loss 2.2994, valid loss 2.3298\n",
      "step 3900: train loss 2.2971, valid loss 2.3305\n",
      "step 4200: train loss 2.2800, valid loss 2.3143\n",
      "step 4500: train loss 2.2730, valid loss 2.3195\n",
      "step 4800: train loss 2.2759, valid loss 2.2976\n",
      "step 5100: train loss 2.2641, valid loss 2.2924\n",
      "step 5400: train loss 2.2364, valid loss 2.2921\n",
      "step 5700: train loss 2.2404, valid loss 2.2723\n",
      "step 6000: train loss 2.2363, valid loss 2.2823\n",
      "step 6300: train loss 2.2380, valid loss 2.2741\n",
      "step 6600: train loss 2.2260, valid loss 2.2745\n",
      "step 6900: train loss 2.2231, valid loss 2.2628\n",
      "step 7200: train loss 2.2194, valid loss 2.2611\n",
      "step 7500: train loss 2.2101, valid loss 2.2595\n",
      "step 7800: train loss 2.2144, valid loss 2.2611\n",
      "step 8100: train loss 2.1993, valid loss 2.2558\n",
      "step 8400: train loss 2.2065, valid loss 2.2525\n",
      "step 8700: train loss 2.2042, valid loss 2.2507\n",
      "step 9000: train loss 2.2049, valid loss 2.2530\n",
      "step 9300: train loss 2.2017, valid loss 2.2566\n",
      "step 9600: train loss 2.1908, valid loss 2.2253\n",
      "step 9900: train loss 2.1885, valid loss 2.2399\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0a1050b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "Gite?\n",
      "\n",
      "Thasefardwe thor's orke the,-ht ce an 'fll cals me. Nurve thou thehaprichis mach yous blo Gray sing bray, siggsire mary swe teras det; his loow? as bavil tall rett swen fall I Ond nonot ist law's slare.\n",
      "\n",
      "KING I'TO:\n",
      "Whear?\n",
      "\n",
      "Of I st:\n",
      "Hot may.\n",
      "\n",
      "Nock? dadd Butwosen.\n",
      "\n",
      "QUEENGRE: there no,, jus; tha\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTIHEAD###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbc85b",
   "metadata": {},
   "source": [
    "# Adding Non-Linearity\n",
    "So far we have just added attention, but we do not use any activation functions. Here we are going to add non-liniarities, but first all the boiler-plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b4042cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3acef638",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea6cc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4d2172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91bf85f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be0cf7",
   "metadata": {},
   "source": [
    "Here we add the FeedForwad class with has a linear layer and uses the ReLU non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34803405",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc284650",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embed//4)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffw(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cddce870",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8b3449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTNONLINEARITY###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a50e247",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1700, valid loss 4.1720\n",
      "step 300: train loss 2.7750, valid loss 2.7878\n",
      "step 600: train loss 2.5964, valid loss 2.6003\n",
      "step 900: train loss 2.5114, valid loss 2.5038\n",
      "step 1200: train loss 2.4553, valid loss 2.4620\n",
      "step 1500: train loss 2.4258, valid loss 2.4423\n",
      "step 1800: train loss 2.3927, valid loss 2.4084\n",
      "step 2100: train loss 2.3843, valid loss 2.3806\n",
      "step 2400: train loss 2.3601, valid loss 2.3699\n",
      "step 2700: train loss 2.3435, valid loss 2.3580\n",
      "step 3000: train loss 2.3271, valid loss 2.3368\n",
      "step 3300: train loss 2.3180, valid loss 2.3239\n",
      "step 3600: train loss 2.3051, valid loss 2.3175\n",
      "step 3900: train loss 2.2926, valid loss 2.3072\n",
      "step 4200: train loss 2.2750, valid loss 2.3021\n",
      "step 4500: train loss 2.2735, valid loss 2.2964\n",
      "step 4800: train loss 2.2559, valid loss 2.2718\n",
      "step 5100: train loss 2.2501, valid loss 2.2915\n",
      "step 5400: train loss 2.2489, valid loss 2.2761\n",
      "step 5700: train loss 2.2311, valid loss 2.2711\n",
      "step 6000: train loss 2.2245, valid loss 2.2700\n",
      "step 6300: train loss 2.2228, valid loss 2.2503\n",
      "step 6600: train loss 2.2082, valid loss 2.2367\n",
      "step 6900: train loss 2.2194, valid loss 2.2480\n",
      "step 7200: train loss 2.2178, valid loss 2.2349\n",
      "step 7500: train loss 2.2016, valid loss 2.2449\n",
      "step 7800: train loss 2.2081, valid loss 2.2433\n",
      "step 8100: train loss 2.1930, valid loss 2.2413\n",
      "step 8400: train loss 2.1981, valid loss 2.2407\n",
      "step 8700: train loss 2.1894, valid loss 2.2361\n",
      "step 9000: train loss 2.1902, valid loss 2.2347\n",
      "step 9300: train loss 2.1800, valid loss 2.2206\n",
      "step 9600: train loss 2.1694, valid loss 2.2111\n",
      "step 9900: train loss 2.1663, valid loss 2.2208\n"
     ]
    }
   ],
   "source": [
    "###GPTNONLINEARITY###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a17665ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "FPRDI:\n",
      "Gould; the your'd bereak.\n",
      "\n",
      "DUCEN NO:\n",
      "Wir for hid mie my\n",
      "They tor the, bus,\n",
      "As wit his thoumy burd whouske.\n",
      "\n",
      "CABESTIN\n",
      "No nou of I st se\n",
      "Whe wou.\n",
      "\n",
      "KE MET:\n",
      "A't. 'BEY:\n",
      "I thryess theme.\n",
      "\n",
      "Mome, andou,\n",
      "Ages.\n",
      "\n",
      "FERBIZIS:\n",
      "He, seU ME vop lillt harsast 'st pill not, lad wo's pisus!\n",
      "\n",
      "BUCILONTESBEO:\n",
      "Wo ait\n"
     ]
    }
   ],
   "source": [
    "###GPTNONLINEARITY###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dd505",
   "metadata": {},
   "source": [
    "# Creating Transformer Block\n",
    "We simply write the transformer code into a block such that we easily can create multiple transformer layers for the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b064408",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef743e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embed = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "edb16540",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69148919",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "114a6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "727f7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0e072aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" Transformer block \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Adding Attention and ffw to X \"\"\"\n",
    "        x = self.sa(x)\n",
    "        x = self.ffw(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b993409",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f4a51235",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4652ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTMULTILAYER###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b7305663",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2121, valid loss 4.2139\n",
      "step 300: train loss 3.2488, valid loss 3.2867\n",
      "step 600: train loss 3.1272, valid loss 3.1155\n",
      "step 900: train loss 2.8783, valid loss 2.8771\n",
      "step 1200: train loss 2.7434, valid loss 2.7289\n",
      "step 1500: train loss 2.6651, valid loss 2.6661\n",
      "step 1800: train loss 2.6052, valid loss 2.5962\n",
      "step 2100: train loss 2.5637, valid loss 2.5549\n",
      "step 2400: train loss 2.5275, valid loss 2.5247\n",
      "step 2700: train loss 2.4861, valid loss 2.4985\n",
      "step 3000: train loss 2.4691, valid loss 2.4583\n",
      "step 3300: train loss 2.4530, valid loss 2.4346\n",
      "step 3600: train loss 2.4373, valid loss 2.4401\n",
      "step 3900: train loss 2.3983, valid loss 2.4056\n",
      "step 4200: train loss 2.4049, valid loss 2.3902\n",
      "step 4500: train loss 2.3889, valid loss 2.3864\n",
      "step 4800: train loss 2.3644, valid loss 2.3709\n",
      "step 5100: train loss 2.3600, valid loss 2.3512\n",
      "step 5400: train loss 2.3526, valid loss 2.3407\n",
      "step 5700: train loss 2.3404, valid loss 2.3582\n",
      "step 6000: train loss 2.3376, valid loss 2.3291\n",
      "step 6300: train loss 2.3264, valid loss 2.3331\n",
      "step 6600: train loss 2.3095, valid loss 2.3229\n",
      "step 6900: train loss 2.2875, valid loss 2.3106\n",
      "step 7200: train loss 2.3334, valid loss 2.3426\n",
      "step 7500: train loss 2.3001, valid loss 2.3159\n",
      "step 7800: train loss 2.2750, valid loss 2.3042\n",
      "step 8100: train loss 2.2714, valid loss 2.3105\n",
      "step 8400: train loss 2.2675, valid loss 2.2823\n",
      "step 8700: train loss 2.2650, valid loss 2.2913\n",
      "step 9000: train loss 2.2432, valid loss 2.2794\n",
      "step 9300: train loss 2.2401, valid loss 2.2666\n",
      "step 9600: train loss 2.2298, valid loss 2.2637\n",
      "step 9900: train loss 2.2394, valid loss 2.2848\n"
     ]
    }
   ],
   "source": [
    "###GPTMULTILAYER###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c552fad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      " \n",
      "A beate of.\n",
      "Andot ca? The, mings: mollavavot thee meast undel!' Circoveen this mor hasher, deoms for, why the bues butfer wown here henplise and the wave wity vate to feraths, where boires thow di teare you reith are,\n",
      "Booy ils ous;\n",
      "Wel dise-arands manow hat ir arlh, I dint tigon: blold, thins maint \n"
     ]
    }
   ],
   "source": [
    "###GPTMULTILAYER###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e1808",
   "metadata": {},
   "source": [
    "# Adding Residual Connections, Normalization and Dropout\n",
    "Here we add the different innovations from the transformer paper:\n",
    "\n",
    "* Residual Connection\n",
    "  * It is basically just adding two matrices together. This is done in the **Block** class two times.\n",
    "* Layer Normalization\n",
    "  * It is very similar to batch normalization ensuring zero mean and 1 variance, it just normalized rows instead of columns. We add it in **Block**.\n",
    "* Dropout\n",
    "  * Randomly prevents some of the nodes from communicating. This has a regularizing effect, as the network is forced to make more robust connections. Implemented in **Head**, **MultiHeadAttention** and **FeedForward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a767557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#################\n",
    "### Libraries ###\n",
    "#################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'; device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f34f3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#######################\n",
    "### Hyperparameters ###\n",
    "#######################\n",
    "batch_size = 32\n",
    "block_size = 16\n",
    "max_steps = 10000\n",
    "eval_interval = 300\n",
    "learning_rate = 3e-3\n",
    "eval_iters = 200\n",
    "n_head = 5\n",
    "n_layer = 5\n",
    "n_embed = 32 \n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "864e7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "############\n",
    "### Data ###\n",
    "############\n",
    "\n",
    "# Reading Data\n",
    "with open(\"../../data/tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# All unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Building the vocabulary\n",
    "ctoi = {s:i for i,s in enumerate(chars)}\n",
    "itoc = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "# Building encoder/decoder\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])\n",
    "\n",
    "# Tokenizing dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Train/Valid Split\n",
    "n = int(0.9*len(data))\n",
    "data_train = data[:n]\n",
    "data_valid = data[n:]\n",
    "\n",
    "# Gets Ramdom Batches\n",
    "def get_batch(split: str, batch_size: int, block_size: int) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates a batch of data of inputs x and targets y.\n",
    "    Inputs:\n",
    "        split: test or valid split\n",
    "        batch_size: How many independent sequences will be processed in parallel\n",
    "        block_size: Maximum context length\n",
    "    Outputs:\n",
    "        x, y: a tuple with xs and ys\n",
    "    \"\"\"\n",
    "    data = data_train if split == 'train' else data_valid\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fbac0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "#############\n",
    "### Model ###\n",
    "#############\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        \"\"\" Creating three linear layers and a mask \"\"\"\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Attention calculation \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 ## **-0.5 normalize variance to 1\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e801445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        \"\"\" Multiple heads in parallel \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Calculating and Concatenating Results \"\"\"\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93741f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "502b1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        \"\"\" Transformer block \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffw = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # Have trainable paramters gamma and beta \n",
    "        self.ln2 = nn.LayerNorm(n_embed) # Have trainable paramters gamma and beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Adding Attention, ffw and pre-layernorm to X \"\"\"\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffw(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e270998",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Creating Layers \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> tuple:\n",
    "        \"\"\" Calculating the Loss \"\"\"\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,embed_size)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Loss function takes (BATCH, CHANNEL, TIME) so we rearrange\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\" Generates Tokens Using a Sliding Window \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9313415",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "# Function for estimating loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Estimates losses on train and valid\n",
    "    Outputs:\n",
    "        out: Mean loss across eval_iters items\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split, batch_size, block_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1322e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "# Creating model\n",
    "model = BigramLanguageModel()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c1028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3294, valid loss 4.3323\n",
      "step 300: train loss 2.3949, valid loss 2.3950\n",
      "step 600: train loss 2.2566, valid loss 2.2590\n",
      "step 900: train loss 2.1788, valid loss 2.1846\n",
      "step 1200: train loss 2.1008, valid loss 2.1441\n",
      "step 1500: train loss 2.0630, valid loss 2.0964\n",
      "step 1800: train loss 2.0359, valid loss 2.0921\n",
      "step 2100: train loss 2.0032, valid loss 2.0599\n",
      "step 2400: train loss 1.9748, valid loss 2.0579\n",
      "step 2700: train loss 1.9587, valid loss 2.0482\n",
      "step 3000: train loss 1.9506, valid loss 2.0228\n",
      "step 3300: train loss 1.9302, valid loss 2.0066\n",
      "step 3600: train loss 1.9142, valid loss 2.0016\n",
      "step 3900: train loss 1.8987, valid loss 2.0026\n",
      "step 4200: train loss 1.9032, valid loss 1.9945\n",
      "step 4500: train loss 1.9038, valid loss 2.0191\n",
      "step 4800: train loss 1.8872, valid loss 1.9736\n",
      "step 5100: train loss 1.8728, valid loss 1.9725\n",
      "step 5400: train loss 1.8685, valid loss 1.9724\n",
      "step 5700: train loss 1.8691, valid loss 1.9703\n",
      "step 6000: train loss 1.8553, valid loss 1.9599\n",
      "step 6300: train loss 1.8437, valid loss 1.9576\n",
      "step 6600: train loss 1.8401, valid loss 1.9447\n",
      "step 6900: train loss 1.8392, valid loss 1.9460\n",
      "step 7200: train loss 1.8410, valid loss 1.9405\n",
      "step 7500: train loss 1.8352, valid loss 1.9422\n",
      "step 7800: train loss 1.8304, valid loss 1.9459\n",
      "step 8100: train loss 1.8217, valid loss 1.9610\n",
      "step 8400: train loss 1.8218, valid loss 1.9486\n",
      "step 8700: train loss 1.8230, valid loss 1.9455\n",
      "step 9000: train loss 1.8141, valid loss 1.9248\n",
      "step 9300: train loss 1.8168, valid loss 1.9508\n",
      "step 9600: train loss 1.8089, valid loss 1.9319\n"
     ]
    }
   ],
   "source": [
    "###GPTCONNECT###\n",
    "################\n",
    "### Training ###\n",
    "################\n",
    "\n",
    "# Creating PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for step in range(max_steps):\n",
    "    \n",
    "    # Once in a while evaluate loss on train and valid sets\n",
    "    if step % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {step}: train loss {losses['train']:.4f}, valid loss {losses['valid']:.4f}\")\n",
    "    \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45880ec3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###GPTCONNECT###\n",
    "##################\n",
    "### Generating ###\n",
    "##################\n",
    "# Generating some text\n",
    "model_input = torch.zeros((1,1), dtype=torch.long) # Input token 0, which is \\n\n",
    "model_output = model.generate(model_input, max_new_tokens=300)[0].tolist()\n",
    "print(f\"Generated Text: \\n {decode(model_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c053d5",
   "metadata": {},
   "source": [
    "# Exporting Code\n",
    "Here we export code labeled with ###BIGRAM###, ###GPTHEAD### and so on to scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###BIGRAM###\" ../../modules/GPT/bigram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTHEAD###\" ../../modules/GPT/GPThead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486fd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTMULTIHEAD###\" ../../modules/GPT/GPTmultihead.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTNONLINEARITY###\" ../../modules/GPT/GPTnonlinearity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTMULTILAYER###\" ../../modules/GPT/GPTmultilayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbac4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 ../../helpers/ipynb_to_py.py 1.\\ Building\\ GPT-like\\ Model.ipynb \"###GPTCONNECT###\" ../../modules/GPT/GPT.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
